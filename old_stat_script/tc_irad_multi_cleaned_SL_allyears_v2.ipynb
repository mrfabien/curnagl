{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "def open_datasets(year, variable, months, way):\n",
    "    datasets = []\n",
    "    for month in months:\n",
    "        filename = f'ERA5_{year}-{month}_{variable}.nc'\n",
    "        dataset = xr.open_dataset(f'{way}{variable}/{filename}')\n",
    "        datasets.append(dataset)\n",
    "    return xr.concat(datasets, dim='time')\n",
    "\n",
    "def process_storm_data(dates, year, dew_point_xr, specific_var):\n",
    "    index_start_october = dates[(dates['start_date'].dt.month >= 10) & (dates['start_date'].dt.year == year)].index[0]\n",
    "    index_end_march = dates[(dates['end_date'].dt.month <= 3) & (dates['end_date'].dt.year == year + 1)].index[-1]\n",
    "\n",
    "    for i in range(index_start_october, index_end_march + 1):\n",
    "        start_date = dates.at[i, 'start_date']\n",
    "        end_date = dates.at[i, 'end_date']\n",
    "        new_dataset = dew_point_xr[specific_var].sel(time=slice(start_date, end_date))\n",
    "        yield new_dataset\n",
    "\n",
    "def main(folder, year):\n",
    "    year = int(year)\n",
    "    year_next = year + 1\n",
    "    month_act = [10, 11, 12]\n",
    "    month_next = [1, 2, 3]\n",
    "    way = '/work/FAC/FGSE/IDYST/tbeucler/default/raw_data/ECMWF/ERA5/SL/'\n",
    "\n",
    "    if year in [1990, 2021]:\n",
    "        months = month_act + month_next if year == 1990 else month_next + month_act\n",
    "        dew_point_xr = open_datasets(str(year), folder, months, way)\n",
    "    else:\n",
    "        dew_point_xr_act = open_datasets(str(year), folder, month_act, way)\n",
    "        dew_point_xr_next = open_datasets(str(year_next), folder, month_next, way)\n",
    "        dew_point_xr = xr.concat([dew_point_xr_act, dew_point_xr_next], dim='time')\n",
    "\n",
    "    specific_var = next(var for var in dew_point_xr.variables if var not in ['longitude', 'latitude', 'time'])\n",
    "    dates = pd.read_csv('/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/curnagl/storms_start_end.csv', parse_dates=['start_date', 'end_date'])\n",
    "    for new_dataset in process_storm_data(dates, year, dew_point_xr, specific_var):\n",
    "        # Process each storm dataset here\n",
    "        pass\n",
    "\n",
    "'''if __name__ == '__main__':\n",
    "    folder = sys.argv[1]\n",
    "    year = sys.argv[2]\n",
    "    main(folder, year)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a dataset\n",
    "year = str(1990)\n",
    "\n",
    "months = [1, 2, 3, 10 ,11, 12]\n",
    "\n",
    "folder = '2m_dewpoint_temperature'\n",
    "\n",
    "way = '/work/FAC/FGSE/IDYST/tbeucler/default/raw_data/ECMWF/ERA5/SL/'\n",
    "\n",
    "test = open_datasets(year, folder, months, way)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_var = next(var for var in test.variables if var not in ['longitude', 'latitude', 'time'])\n",
    "dates = pd.read_csv('/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/curnagl/storms_start_end.csv')\n",
    "\n",
    "test_process = process_storm_data(dates, year, test, 'd2m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Define a function to open datasets and concatenate them\n",
    "def open_and_concatenate(year, variable, months, way):\n",
    "    datasets = [xr.open_dataset(f'{way}{variable}/ERA5_{year}-{month}_{variable}.nc') for month in months]\n",
    "    return xr.concat(datasets, dim='time')\n",
    "\n",
    "# Define a function to calculate statistics\n",
    "def calculate_statistics(data_array):\n",
    "    return {\n",
    "        'mean': np.mean(data_array),\n",
    "        'min': np.min(data_array),\n",
    "        'max': np.max(data_array),\n",
    "        'std': np.std(data_array),\n",
    "        'skew': pd.Series(data_array.reshape(-1)).skew(),\n",
    "        'kurtosis': pd.Series(data_array.reshape(-1)).kurtosis()\n",
    "    }\n",
    "\n",
    "# Function to log processing details\n",
    "def log_processing(variable, year):\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    log_message = f'Processed variable: {variable}, Year: {year}, Timestamp: {timestamp}'\n",
    "    with open(f'/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/curnagl/datasets/{variable}/processing_log.txt', 'a') as log_file:\n",
    "        log_file.write(log_message + '\\n')\n",
    "\n",
    "# Main function to process data\n",
    "def process_data(variable, year):\n",
    "    year = int(year)\n",
    "    year_next = year + 1\n",
    "    month_act = [10, 11, 12]\n",
    "    month_next = [1, 2, 3]\n",
    "    way = '/work/FAC/FGSE/IDYST/tbeucler/default/raw_data/ECMWF/ERA5/SL/'\n",
    "\n",
    "    # Open and concatenate datasets\n",
    "    if year in [1990, 2021]:\n",
    "        months = month_act + month_next if year == 1990 else month_next + month_act\n",
    "        dataset = open_and_concatenate(str(year), variable, months, way)\n",
    "    else:\n",
    "        dataset_act = open_and_concatenate(str(year), variable, month_act, way)\n",
    "        dataset_next = open_and_concatenate(str(year_next), variable, month_next, way)\n",
    "        dataset = xr.concat([dataset_act, dataset_next], dim='time')\n",
    "\n",
    "    # Determine the specific variable to extract\n",
    "    specific_var = next(var for var in dataset.variables if var not in ['longitude', 'latitude', 'time'])\n",
    "\n",
    "    # Import all tracks and convert dates\n",
    "    dates = pd.read_csv(f'/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/curnagl/storms_start_end.csv', parse_dates=['start_date', 'end_date'])\n",
    "    dates['year'] = dates['start_date'].dt.year\n",
    "\n",
    "    # Find the indices for storms within the specified timeframe\n",
    "    if year == 1990:\n",
    "        index_start_march = dates[(dates['start_date'].dt.month >= 10) & (dates['year'] == year)].index[0]\n",
    "        index_end_october = dates[(dates['end_date'].dt.month <= 3) & (dates['year'] == year)].index[0]\n",
    "    else:\n",
    "        index_start_october = dates[(dates['start_date'].dt.month >= 10) & (dates['year'] == year)].index[0]\n",
    "        index_end_march = dates[(dates['end_date'].dt.month <= 3) & (dates['year'] == year_next)].index[-1]\n",
    "\n",
    "    # Process each storm\n",
    "    for i in range(index_start_october, index_end_march + 1):\n",
    "        track = pd.read_csv(f'/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/curnagl/tc_irad_tracks/tc_1_hour/tc_irad_{i+1}_interp.txt')\n",
    "        start_date = dates.at[i, 'start_date']\n",
    "        end_date = dates.at[i, 'end_date']\n",
    "        storm_data = dataset[specific_var].sel(time=slice(start_date, end_date))\n",
    "\n",
    "        # Initialize lists to store statistics\n",
    "        stats = {'mean': [], 'min': [], 'max': [], 'std': [], 'skew': [], 'kurtosis': []}\n",
    "\n",
    "        # Calculate statistics for each time step\n",
    "        for time_step in storm_data.time:\n",
    "            data_slice = storm_data.sel(time=time_step).values\n",
    "            step_stats = calculate_statistics(data_slice)\n",
    "            for key in stats:\n",
    "                stats[key].append(step_stats[key])\n",
    "\n",
    "        # Save statistics to CSV files\n",
    "        for key in stats:\n",
    "            pd.DataFrame(stats[key]).to_csv(f'/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/curnagl/datasets/{variable}/storm_{i+1}/{key}_{i+1}.csv')\n",
    "\n",
    "    # Log the processing details\n",
    "    log_processing(variable, year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests for 1990\n",
    "variable = '2m_dewpoint_temperature'\n",
    "year = 2021\n",
    "year_next = year + 1\n",
    "month_act = [10, 11, 12]\n",
    "month_next = [1, 2, 3]\n",
    "way = '/work/FAC/FGSE/IDYST/tbeucler/default/raw_data/ECMWF/ERA5/SL/'\n",
    "\n",
    "# Open and concatenate datasets\n",
    "if year == 1990:\n",
    "    dataset_act = open_and_concatenate(str(year), variable, month_next, way)\n",
    "    dataset_next = open_and_concatenate(str(year_next), variable, month_next, way)\n",
    "    dataset = xr.concat([dataset_act, dataset_next], dim='time')\n",
    "elif year == 2021:\n",
    "    dataset = open_and_concatenate(str(year), variable, month_next, way)\n",
    "else:\n",
    "    dataset_act = open_and_concatenate(str(year), variable, month_act, way)\n",
    "    dataset_next = open_and_concatenate(str(year_next), variable, month_next, way)\n",
    "    dataset = xr.concat([dataset_act, dataset_next], dim='time')\n",
    "\n",
    "# Determine the specific variable to extract\n",
    "specific_var = next(var for var in dataset.variables if var not in ['longitude', 'latitude', 'time'])\n",
    "\n",
    "# Import all tracks and convert dates\n",
    "dates = pd.read_csv(f'/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/curnagl/storms_start_end.csv', parse_dates=['start_date', 'end_date'])\n",
    "dates['year'] = dates['start_date'].dt.year\n",
    "\n",
    "# Find the indices for storms within the specified timeframe\n",
    "if year == 1990:\n",
    "    index_start_october = dates[(dates['start_date'].dt.month <= 3) & (dates['start_date'].dt.year == year)].index[0]\n",
    "    index_end_march = dates[(dates['end_date'].dt.month <= 3) & (dates['end_date'].dt.year == year_next)].index[0]\n",
    "elif year == 2021:\n",
    "    index_start_october = dates[(dates['start_date'].dt.month <= 3) & (dates['start_date'].dt.year == year)].index[0]\n",
    "    index_end_march = dates[(dates['end_date'].dt.year == 2021)].index[0]\n",
    "else:\n",
    "    index_start_october = dates[(dates['start_date'].dt.month >= 10) & (dates['start_date'].dt.year == year)].index[0]\n",
    "    index_end_march = dates[(dates['end_date'].dt.month <= 3) & (dates['end_date'].dt.year == year_next)].index[0]\n",
    "\n",
    "# Process each storm\n",
    "for i in range(index_start_october, index_end_march + 1):\n",
    "    track = pd.read_csv(f'/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/curnagl/tc_irad_tracks/tc_1_hour/tc_irad_{i+1}_interp.txt')\n",
    "    start_date = dates.at[i, 'start_date']\n",
    "    end_date = dates.at[i, 'end_date']\n",
    "    storm_data = dataset[specific_var].sel(time=slice(start_date, end_date))\n",
    "\n",
    "    # Initialize lists to store statistics\n",
    "    stats = {'mean': [], 'min': [], 'max': [], 'std': []}\n",
    "    #, 'skew': [], 'kurtosis': []\n",
    "\n",
    "    # Calculate statistics for each time step\n",
    "    for time_step in storm_data.time:\n",
    "        data_slice = storm_data.sel(time=time_step).values\n",
    "        step_stats = calculate_statistics(data_slice)\n",
    "        for key in stats:\n",
    "            stats[key].append(step_stats[key])\n",
    "\n",
    "    # Save statistics to CSV files\n",
    "    for key in stats:\n",
    "        pd.DataFrame(stats[key]).to_csv(f'/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/curnagl/datasets/{variable}/storm_{i+1}/{key}_{i+1}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_end_march = dates[(dates['end_date'].dt.year == 2021)].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 1990\n",
    "year_next = 1991\n",
    "#index_start_october = dates[(dates['start_date'].dt.month <= 3) & (dates['start_date'].dt.year == year)].index[0]\n",
    "#index_end_march = dates[(dates['end_date'].dt.month <= 3) & (dates['end_date'].dt.year == year_next)].index[0]\n",
    "\n",
    "if year == 1990:\n",
    "    index_start_october = dates[(dates['start_date'].dt.month <= 3) & (dates['start_date'].dt.year == year)].index[0]\n",
    "    index_end_march = dates[(dates['end_date'].dt.month <= 3) & (dates['end_date'].dt.year == year_next)].index[0]-1\n",
    "else:\n",
    "    index_start_october = dates[(dates['start_date'].dt.month >= 10) & (dates['start_date'].dt.year == year)].index[0]\n",
    "    index_end_march = dates[(dates['end_date'].dt.month <= 3) & (dates['end_date'].dt.year == year_next)].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv[1]='2m_dewpoint_temperature'\n",
    "sys.argv[2]='1990'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    variable = sys.argv[1]\n",
    "    year = sys.argv[2]\n",
    "    process_data(variable, year)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
