{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxr\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/kera_lgbm/lib/python3.9/site-packages/xarray/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m testing, tutorial\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     load_dataarray,\n\u001b[1;32m      4\u001b[0m     load_dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     save_mfdataset,\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mzarr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m open_zarr\n",
      "File \u001b[0;32m~/miniconda3/envs/kera_lgbm/lib/python3.9/site-packages/xarray/testing.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Hashable\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Union\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m duck_array_ops, formatting, utils\n",
      "File \u001b[0;32m~/miniconda3/envs/kera_lgbm/lib/python3.9/site-packages/numpy/__init__.py:151\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linalg\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fft\n\u001b[0;32m--> 151\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m polynomial\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m random\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ctypeslib\n",
      "File \u001b[0;32m~/miniconda3/envs/kera_lgbm/lib/python3.9/site-packages/numpy/polynomial/__init__.py:119\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchebyshev\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chebyshev\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegendre\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Legendre\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhermite\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Hermite\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhermite_e\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HermiteE\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlaguerre\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Laguerre\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:846\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:941\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1040\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from guppy import hpy\n",
    "\n",
    "# Create an instance of heapy\n",
    "hp = hpy()\n",
    "\n",
    "# Define a function to open datasets and concatenate them\n",
    "def open_and_concatenate(year, variable, months, way, level=0):\n",
    "    datasets = [xr.open_dataset(f'{way}{variable}/ERA5_{year}-{month}_{variable}.nc') for month in months]\n",
    "    if variable == 'geopotential' and level != 0:\n",
    "        datasets = [dataset.sel(level=level) for dataset in datasets]\n",
    "    return xr.concat(datasets, dim='time')\n",
    "\n",
    "# Define a function to calculate statistics\n",
    "def calculate_statistics(data_array):\n",
    "    return {\n",
    "        'mean': np.mean(data_array),\n",
    "        'min': np.min(data_array),\n",
    "        'max': np.max(data_array),\n",
    "        'std': np.std(data_array),\n",
    "    }\n",
    "\n",
    "# Function to log processing details\n",
    "def log_processing(variable, year, level, storm_number):\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    log_message = f'Processed variable: {variable}, Year: {year}, Level: {level}, Timestamp: {timestamp}, Storm number:{storm_number}'\n",
    "    with open(f'/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/curnagl/datasets/processing_log.txt', 'a') as log_file:\n",
    "        log_file.write(log_message + '\\n')\n",
    "\n",
    "# Main function to process data\n",
    "def process_data(variable, year, level=0):\n",
    "    year = int(year)\n",
    "    year_next = year + 1\n",
    "    month_act = [10, 11, 12]\n",
    "    month_next = [1, 2, 3]\n",
    "    if variable == 'geopotential':\n",
    "        way = '/work/FAC/FGSE/IDYST/tbeucler/default/raw_data/ECMWF/ERA5_hourly_PL/'\n",
    "    else:\n",
    "        way = '/work/FAC/FGSE/IDYST/tbeucler/default/raw_data/ECMWF/ERA5/SL/'\n",
    "\n",
    "    # Open and concatenate datasets\n",
    "    if year == 1990:\n",
    "        dataset_act = open_and_concatenate(str(year), variable, month_next, way, level)\n",
    "        dataset_next = open_and_concatenate(str(year_next), variable, month_next, way, level)\n",
    "        dataset = xr.concat([dataset_act, dataset_next], dim='time')\n",
    "        dataset = dataset.chunk({'time': 10})\n",
    "    elif year == 2021:\n",
    "        dataset = open_and_concatenate(str(year), variable, month_next, way, level)\n",
    "    else:\n",
    "        dataset_act = open_and_concatenate(str(year), variable, month_act, way, level)\n",
    "        dataset_next = open_and_concatenate(str(year_next), variable, month_next, way, level)\n",
    "        dataset = xr.concat([dataset_act, dataset_next], dim='time')\n",
    "        dataset = dataset.chunk({'time': 10})\n",
    "\n",
    "    # Determine the specific variable to extract\n",
    "    specific_var = next(var for var in dataset.variables if var not in ['longitude', 'latitude', 'time', 'level'])\n",
    "\n",
    "    # Import all tracks and convert dates\n",
    "    dates = pd.read_csv(f'/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/curnagl/storms_start_end.csv', parse_dates=['start_date', 'end_date'])\n",
    "    dates['year'] = dates['start_date'].dt.year\n",
    "\n",
    "    # Find the indices for storms within the specified timeframe\n",
    "    if year == 1990:\n",
    "        index_start_october = dates[(dates['start_date'].dt.month <= 3) & (dates['start_date'].dt.year == year)].index[0]\n",
    "        index_end_march = dates[(dates['end_date'].dt.month <= 3) & (dates['end_date'].dt.year == year_next)].index[0]\n",
    "    elif year == 2021:\n",
    "        index_start_october = dates[(dates['start_date'].dt.month <= 3) & (dates['start_date'].dt.year == year)].index[0]\n",
    "        index_end_march = dates[(dates['end_date'].dt.year == 2021)].index[0]\n",
    "    else:\n",
    "        index_start_october = dates[((dates['start_date'].dt.month >= 10) & (dates['start_date'].dt.year == year)) | ((dates['start_date'].dt.year == year_next) & (dates['start_date'].dt.month >= 1))].index[0]\n",
    "        index_end_march_first = dates[((dates['end_date'].dt.month <= 3) & (dates['end_date'].dt.year == year_next))].index\n",
    "        if len(index_end_march_first) > 0:\n",
    "            index_end_march = index_end_march_first[-1]\n",
    "        else:\n",
    "            index_end_march = dates[((dates['end_date'].dt.year == year) & (dates['end_date'].dt.month <= 12))].index[-1]\n",
    "\n",
    "    # Process each storm\n",
    "    for i in range(index_start_october, index_end_march + 1):\n",
    "        track = pd.read_csv(f'/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/curnagl/tc_irad_tracks/tc_1_hour/tc_irad_{i+1}_interp.txt')\n",
    "        start_date = dates.at[i, 'start_date']\n",
    "        end_date = dates.at[i, 'end_date']\n",
    "        storm_data = dataset[specific_var].sel(time=slice(start_date, end_date))\n",
    "\n",
    "        # Initialize lists to store statistics\n",
    "        stats = {'mean': [], 'min': [], 'max': [], 'std': []}\n",
    "\n",
    "        # Process each time step\n",
    "        for t_index, time_step in enumerate(storm_data.time):\n",
    "            #data_slice = storm_data.sel(time=time_step).values\n",
    "\n",
    "            # Extract coordinates for the current time step\n",
    "            lon_e_temp, lon_w_temp, lat_s_temp, lat_n_temp = track.iloc[t_index]\n",
    "            lon_test = np.asanyarray(storm_data.longitude[:])\n",
    "            lat_test = np.asanyarray(storm_data.latitude[:])\n",
    "\n",
    "            closest_lon_w = np.abs(lon_test - lon_w_temp).argmin()\n",
    "            closest_lon_e = np.abs(lon_test - lon_e_temp).argmin()\n",
    "            closest_lat_s = np.abs(lat_test - lat_s_temp).argmin()\n",
    "            closest_lat_n = np.abs(lat_test - lat_n_temp).argmin()\n",
    "\n",
    "            closest_lon_w_coor = lon_test[closest_lon_w]\n",
    "            closest_lon_e_coor = lon_test[closest_lon_e]\n",
    "            closest_lat_s_coor = lat_test[closest_lat_s]\n",
    "            closest_lat_n_coor = lat_test[closest_lat_n]\n",
    "\n",
    "            # Use .roll to handle the 0°/360° boundary\n",
    "            if closest_lon_w_coor < 100 and closest_lon_e_coor > 100:\n",
    "                roll_shift = {'longitude': int(round(closest_lon_w_coor, 0)), 'longitude': int(round(closest_lon_e_coor, 0))}\n",
    "                storm_data_rolled = storm_data.roll(roll_shift, roll_coords=True)\n",
    "            else:\n",
    "                storm_data_rolled = storm_data\n",
    "\n",
    "            # Slice the dataset based on the rolled longitudes and latitudes\n",
    "            temp_ds_time = storm_data_rolled.isel(time=t_index)#[specific_var].isel(time=t_index)\n",
    "            temp_ds = temp_ds_time.sel(latitude=slice(closest_lat_n_coor, closest_lat_s_coor),\n",
    "                                       longitude=slice(closest_lon_e_coor, closest_lon_w_coor)).values\n",
    "\n",
    "            # Calculate statistics for the sliced data\n",
    "            step_stats = calculate_statistics(temp_ds)\n",
    "            for key in stats:\n",
    "                stats[key].append(step_stats[key])\n",
    "\n",
    "        # Save statistics to CSV files\n",
    "        for key in stats:\n",
    "            pd.DataFrame(stats[key]).to_csv(f'/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/curnagl/datasets/{variable}/storm_{i+1}/{key}_{i+1}_{level}.csv')\n",
    "\n",
    "    # Log the processing details\n",
    "    log_processing(variable, year, level, i+1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    variable = '2m_temperature'\n",
    "    year = 2006\n",
    "    level = 0\n",
    "    process_data(variable, year, level)\n",
    "\n",
    "# Obtain a snapshot of memory usage\n",
    "h = hp.heap()\n",
    "\n",
    "# Print memory usage information\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = int(year)\n",
    "year_next = year + 1\n",
    "month_act = [10, 11, 12]\n",
    "month_next = [1, 2, 3]\n",
    "if variable == 'geopotential':\n",
    "    way = '/work/FAC/FGSE/IDYST/tbeucler/default/raw_data/ECMWF/ERA5_hourly_PL/'\n",
    "else:\n",
    "    way = '/work/FAC/FGSE/IDYST/tbeucler/default/raw_data/ECMWF/ERA5/SL/'\n",
    "\n",
    "# Open and concatenate datasets\n",
    "if year == 1990:\n",
    "    dataset_act = open_and_concatenate(str(year), variable, month_next, way, level)\n",
    "    dataset_next = open_and_concatenate(str(year_next), variable, month_next, way, level)\n",
    "    dataset = xr.concat([dataset_act, dataset_next], dim='time')\n",
    "    dataset = dataset.chunk({'time': 10})\n",
    "elif year == 2021:\n",
    "    dataset = open_and_concatenate(str(year), variable, month_next, way, level)\n",
    "else:\n",
    "    dataset_act = open_and_concatenate(str(year), variable, month_act, way, level)\n",
    "    dataset_next = open_and_concatenate(str(year_next), variable, month_next, way, level)\n",
    "    dataset = xr.concat([dataset_act, dataset_next], dim='time')\n",
    "    dataset = dataset.chunk({'time': 10})\n",
    "\n",
    "# Determine the specific variable to extract\n",
    "specific_var = next(var for var in dataset.variables if var not in ['longitude', 'latitude', 'time', 'level'])\n",
    "\n",
    "# Import all tracks and convert dates\n",
    "dates = pd.read_csv(f'/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/curnagl/storms_start_end.csv', parse_dates=['start_date', 'end_date'])\n",
    "dates['year'] = dates['start_date'].dt.year\n",
    "\n",
    "# Find the indices for storms within the specified timeframe\n",
    "if year == 1990:\n",
    "    index_start_october = dates[(dates['start_date'].dt.month <= 3) & (dates['start_date'].dt.year == year)].index[0]\n",
    "    index_end_march = dates[(dates['end_date'].dt.month <= 3) & (dates['end_date'].dt.year == year_next)].index[0]\n",
    "elif year == 2021:\n",
    "    index_start_october = dates[(dates['start_date'].dt.month <= 3) & (dates['start_date'].dt.year == year)].index[0]\n",
    "    index_end_march = dates[(dates['end_date'].dt.year == 2021)].index[0]\n",
    "else:\n",
    "    index_start_october = dates[((dates['start_date'].dt.month >= 10) & (dates['start_date'].dt.year == year)) | ((dates['start_date'].dt.year == year_next) & (dates['start_date'].dt.month >= 1))].index[0]\n",
    "    index_end_march_first = dates[((dates['end_date'].dt.month <= 3) & (dates['end_date'].dt.year == year_next))].index\n",
    "    if len(index_end_march_first) > 0:\n",
    "        index_end_march = index_end_march_first[-1]\n",
    "    else:\n",
    "        index_end_march = dates[((dates['end_date'].dt.year == year) & (dates['end_date'].dt.month <= 12))].index[-1]\n",
    "\n",
    "# Process each storm\n",
    "for i in range(index_start_october, index_end_march+1):\n",
    "    track = pd.read_csv(f'/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/curnagl/tc_irad_tracks/tc_1_hour/tc_irad_{i+1}_interp.txt')\n",
    "    start_date = dates.at[i, 'start_date']\n",
    "    end_date = dates.at[i, 'end_date']\n",
    "    storm_data = dataset[specific_var].sel(time=slice(start_date, end_date))\n",
    "\n",
    "    # Initialize lists to store statistics\n",
    "    stats = {'mean': [], 'min': [], 'max': [], 'std': []}\n",
    "\n",
    "    # Process each time step\n",
    "    for t_index, time_step in enumerate(storm_data.time):\n",
    "        #data_slice = storm_data.sel(time=time_step).values\n",
    "\n",
    "        # Extract coordinates for the current time step\n",
    "        lon_e_temp, lon_w_temp, lat_s_temp, lat_n_temp = track.iloc[t_index]\n",
    "        lon_test = np.asanyarray(storm_data.longitude[:])\n",
    "        lat_test = np.asanyarray(storm_data.latitude[:])\n",
    "\n",
    "        closest_lon_w = np.abs(lon_test - lon_w_temp).argmin()\n",
    "        closest_lon_e = np.abs(lon_test - lon_e_temp).argmin()\n",
    "        closest_lat_s = np.abs(lat_test - lat_s_temp).argmin()\n",
    "        closest_lat_n = np.abs(lat_test - lat_n_temp).argmin()\n",
    "\n",
    "        closest_lon_w_coor = lon_test[closest_lon_w]\n",
    "        closest_lon_e_coor = lon_test[closest_lon_e]\n",
    "        closest_lat_s_coor = lat_test[closest_lat_s]\n",
    "        closest_lat_n_coor = lat_test[closest_lat_n]\n",
    "\n",
    "        # Use .roll to handle the 0°/360° boundary\n",
    "        if closest_lon_w_coor < 100 and closest_lon_e_coor > 100:\n",
    "            roll_shift = {'longitude': int(round(closest_lon_w_coor, 0)), 'longitude': int(round(closest_lon_e_coor, 0))}\n",
    "            storm_data_rolled = storm_data.roll(roll_shift, roll_coords=True)\n",
    "        else:\n",
    "            storm_data_rolled = storm_data\n",
    "\n",
    "        # Slice the dataset based on the rolled longitudes and latitudes\n",
    "        temp_ds_time = storm_data_rolled.isel(time=t_index)#[specific_var].isel(time=t_index)\n",
    "        temp_ds = temp_ds_time.sel(latitude=slice(closest_lat_n_coor, closest_lat_s_coor),\n",
    "                                    longitude=slice(closest_lon_e_coor, closest_lon_w_coor)).values\n",
    "\n",
    "        # Calculate statistics for the sliced data\n",
    "        step_stats = calculate_statistics(temp_ds)\n",
    "        for key in stats:\n",
    "            stats[key].append(step_stats[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"if not all_csv_files_exist(variable, year, level):\\n    process_data(variable, year, level)\\nelse:\\n    print(f'All CSV files for variable: {variable}, year: {year}, and level: {level} already exist. Skipping processing.')\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new tests need to be done\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "dataset_path = '/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/curnagl/DATASETS/datasets_3h'\n",
    "track_path = '/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/curnagl/ALL_TRACKS/tracks_3h'\n",
    "\n",
    "# Works for all years\n",
    "# Define a function to open datasets and concatenate them\n",
    "def open_and_concatenate(year, variable, months, way, level=0):\n",
    "    datasets = []\n",
    "    for month in months:\n",
    "        dataset = xr.open_dataset(f'{way}{variable}/ERA5_{year}-{month}_{variable}.nc')\n",
    "        if variable == 'geopotential' and level != 0:\n",
    "            dataset = dataset.sel(level=level)\n",
    "        \n",
    "        # Create a date range with 3-hour intervals starting from midnight\n",
    "        start = pd.Timestamp(f\"{year}-{month}-01 00:00:00\")\n",
    "        if month == 12:\n",
    "            end = pd.date_range(start=f\"{year}-{month}-01\", end=f\"{str(int(year)+1)}-01-01\", freq='M')[0] + pd.Timedelta(hours=21)\n",
    "        else:\n",
    "            end = pd.date_range(start=f\"{year}-{month}-01\", end=f\"{year}-{month+1}-01\", freq='M')[0] + pd.Timedelta(hours=21)\n",
    "        date_range = pd.date_range(start, end, freq='3H')\n",
    "\n",
    "        # Select the data at the specific timesteps\n",
    "        dataset = dataset.sel(time=date_range)\n",
    "        \n",
    "        datasets.append(dataset)\n",
    "        dataset.close()\n",
    "\n",
    "    return xr.concat(datasets, dim='time')\n",
    "\n",
    "# Define a function to calculate statistics\n",
    "def calculate_statistics(data_array):\n",
    "    return {\n",
    "        'mean': np.mean(data_array),\n",
    "        'min': np.min(data_array),\n",
    "        'max': np.max(data_array),\n",
    "        'std': np.std(data_array),\n",
    "    }\n",
    "\n",
    "# Function to log processing details\n",
    "def log_processing(variable, year, level, storm_number):\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    log_message = f'Processed variable: {variable}, Year: {year}, Level: {level}, Timestamp: {timestamp}, Storm number:{storm_number}'\n",
    "    with open(f'{dataset_path}/processing_log_3h.txt', 'a') as log_file:\n",
    "        log_file.write(log_message + '\\n')\n",
    "\n",
    "# Function to check if all CSV files exist\n",
    "def all_csv_files_exist(variable, year, level):\n",
    "    directory = f'{dataset_path}/{variable}'\n",
    "#    if not os.path.exists(directory):\n",
    "#       return False\n",
    "\n",
    "    for storm_dir in os.listdir(directory):\n",
    "        storm_path = os.path.join(directory, storm_dir)\n",
    "        if os.path.isdir(storm_path):\n",
    "            for stat in ['mean', 'min', 'max', 'std']:\n",
    "                file_path = os.path.join(storm_path, f'{stat}_{storm_dir.split(\"_\")[1]}_{level}.csv')\n",
    "                if not os.path.exists(file_path):\n",
    "                    return False\n",
    "    return True\n",
    "\n",
    "# Main function to process data\n",
    "def process_data(variable, year, level=0):\n",
    "    year = int(year)\n",
    "    year_next = year + 1\n",
    "    month_act = [10, 11, 12]\n",
    "    month_next = [1, 2, 3]\n",
    "    if variable == 'geopotential':\n",
    "        way = '/work/FAC/FGSE/IDYST/tbeucler/default/raw_data/ECMWF/ERA5_hourly_PL/'\n",
    "    else:\n",
    "        way = '/work/FAC/FGSE/IDYST/tbeucler/default/raw_data/ECMWF/ERA5/SL/'\n",
    "\n",
    "    # Open and concatenate datasets\n",
    "    if year == 1990:\n",
    "        dataset_act = open_and_concatenate(str(year), variable, month_next, way, level)\n",
    "        dataset_next = open_and_concatenate(str(year_next), variable, month_next, way, level)\n",
    "        dataset = xr.concat([dataset_act, dataset_next], dim='time')\n",
    "        dataset = dataset.chunk({'time': 10})\n",
    "    elif year == 2021:\n",
    "        dataset = open_and_concatenate(str(year), variable, month_next, way, level)\n",
    "    else:\n",
    "        dataset_act = open_and_concatenate(str(year), variable, month_act, way, level)\n",
    "        dataset_next = open_and_concatenate(str(year_next), variable, month_next, way, level)\n",
    "        dataset = xr.concat([dataset_act, dataset_next], dim='time')\n",
    "        dataset = dataset.chunk({'time': 10})\n",
    "\n",
    "    # Determine the specific variable to extract\n",
    "    specific_var = next(var for var in dataset.variables if var not in ['longitude', 'latitude', 'time', 'level'])\n",
    "\n",
    "    # Import all tracks and convert dates\n",
    "    dates = pd.read_csv(f'/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/curnagl/storms_start_end_steps.csv', parse_dates=['start_date', 'end_date'])\n",
    "    dates['year'] = dates['start_date'].dt.year\n",
    "\n",
    "    # Find the indices for storms within the specified timeframe\n",
    "    if year == 1990:\n",
    "        index_start_october = dates[(dates['start_date'].dt.month <= 3) & (dates['start_date'].dt.year == year)].index[0]\n",
    "        index_end_march = dates[(dates['end_date'].dt.month <= 3) & (dates['end_date'].dt.year == year_next)].index[0]\n",
    "    elif year == 2021:\n",
    "        index_start_october = dates[(dates['start_date'].dt.month <= 3) & (dates['start_date'].dt.year == year)].index[0]\n",
    "        index_end_march = dates[(dates['end_date'].dt.year == 2021)].index[0]\n",
    "    else:\n",
    "    # Chercher start_october dans year, sinon chercher dès janvier de year_next\n",
    "        index_start_october = dates[((dates['start_date'].dt.month >= 10) & (dates['start_date'].dt.year == year)) | ((dates['start_date'].dt.year == year_next) & (dates['start_date'].dt.month >= 1))].index[0]\n",
    "        index_end_march_first = dates[((dates['end_date'].dt.month <= 3) & (dates['end_date'].dt.year == year_next))].index\n",
    "        #print(index_start_october, index_end_march_first, '3rd condition start_october + index_end_march_first')\n",
    "        if len(index_end_march_first) > 0:\n",
    "            index_end_march = index_end_march_first[-1]\n",
    "            #print(index_end_march, 'index_end_march 1st condition of 2nd condition')\n",
    "        else:\n",
    "            # Si year_next ne renvoie rien, chercher la dernière instance de tempête dans year\n",
    "            index_end_march = dates[((dates['end_date'].dt.year == year) & (dates['end_date'].dt.month <= 12))].index[-1]\n",
    "            #print(index_end_march, 'index_end_march 2nd condition of 2nd condition')\n",
    "    # Process each storm\n",
    "    for i in range(index_start_october, index_end_march + 1):\n",
    "        track = pd.read_csv(f'{track_path}/storm_{i+1}.csv')\n",
    "        start_date = dates.at[i, 'start_date']\n",
    "        end_date = dates.at[i, 'end_date']\n",
    "        storm_data = dataset[specific_var].sel(time=slice(start_date, end_date))\n",
    "\n",
    "        # Initialize lists to store statistics\n",
    "        stats = {'mean': [], 'min': [], 'max': [], 'std': []}\n",
    "        #, 'skewness': [], 'kurtosis': []\n",
    "        \n",
    "        # Process each time step\n",
    "        for t_index in range(0, len(storm_data.time)):#, time_step in enumerate(storm_data.time):\n",
    "            #data_slice = storm_data.sel(time=time_step).values\n",
    "\n",
    "            # Extract coordinates for the current time step\n",
    "            lon_e_temp = track.loc[t_index, 'lon_east']\n",
    "            lon_w_temp = track.loc[t_index, 'lon_west']\n",
    "            lat_s_temp = track.loc[t_index, 'lat_south']\n",
    "            lat_n_temp = track.loc[t_index, 'lat_north']\n",
    "            \n",
    "            lon_test = np.asanyarray(storm_data.longitude[:])\n",
    "            lat_test = np.asanyarray(storm_data.latitude[:])\n",
    "\n",
    "            closest_lon_w = np.abs(lon_test - lon_w_temp).argmin()\n",
    "            closest_lon_e = np.abs(lon_test - lon_e_temp).argmin()\n",
    "            closest_lat_s = np.abs(lat_test - lat_s_temp).argmin()\n",
    "            closest_lat_n = np.abs(lat_test - lat_n_temp).argmin()\n",
    "\n",
    "            closest_lon_w_coor = lon_test[closest_lon_w]\n",
    "            closest_lon_e_coor = lon_test[closest_lon_e]\n",
    "            closest_lat_s_coor = lat_test[closest_lat_s]\n",
    "            closest_lat_n_coor = lat_test[closest_lat_n]\n",
    "\n",
    "            # Use .roll to handle the 0°/360° boundary\n",
    "            if closest_lon_w_coor < 100 and closest_lon_e_coor > 100:\n",
    "                roll_shift = {'longitude': int(round(closest_lon_w_coor, 0)), 'longitude': int(round(closest_lon_e_coor, 0))}\n",
    "                storm_data_rolled = storm_data.roll(roll_shift, roll_coords=True)\n",
    "            else:\n",
    "                storm_data_rolled = storm_data\n",
    "\n",
    "            # Slice the dataset based on the rolled longitudes and latitudes\n",
    "            temp_ds_time = storm_data_rolled.isel(time=t_index)#[specific_var].isel(time=t_index)\n",
    "            temp_ds = temp_ds_time.sel(latitude=slice(closest_lat_n_coor, closest_lat_s_coor),\n",
    "                                       longitude=slice(closest_lon_e_coor, closest_lon_w_coor)).values\n",
    "\n",
    "            # Calculate statistics for the sliced data\n",
    "            step_stats = calculate_statistics(temp_ds)\n",
    "            for key in stats:\n",
    "                stats[key].append(step_stats[key])\n",
    "                \n",
    "        # Save statistics to CSV files\n",
    "        for key in stats:\n",
    "            directory = f'{dataset_path}/{variable}/storm_{i+1}'\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            pd.DataFrame(stats[key]).to_csv(f'{directory}/{key}_{i+1}_{level}.csv')\n",
    "\n",
    "        # Log the processing details\n",
    "        log_processing(variable, year, level, i+1)\n",
    "\n",
    "'''if __name__ == '__main__':\n",
    "    variable = sys.argv[1]\n",
    "    year = sys.argv[2]\n",
    "    level = int(sys.argv[3])\n",
    "    process_data(variable, year, level)'''\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "variable = '2m_temperature'\n",
    "year = 2013\n",
    "level = 0\n",
    "\n",
    "process_data(variable, year, level)\n",
    "\n",
    "'''if not all_csv_files_exist(variable, year, level):\n",
    "    process_data(variable, year, level)\n",
    "else:\n",
    "    print(f'All CSV files for variable: {variable}, year: {year}, and level: {level} already exist. Skipping processing.')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Failed to decode variable '2t': module 'numpy' has no attribute 'matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/kera_lgbm/lib/python3.9/site-packages/xarray/conventions.py:427\u001b[0m, in \u001b[0;36mdecode_cf_variables\u001b[0;34m(variables, attributes, concat_characters, mask_and_scale, decode_times, decode_coords, drop_variables, use_cftime, decode_timedelta)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     new_vars[k] \u001b[38;5;241m=\u001b[39m \u001b[43mdecode_cf_variable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconcat_characters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcat_characters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_and_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_and_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstack_char_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstack_char_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cftime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cftime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_timedelta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_timedelta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/kera_lgbm/lib/python3.9/site-packages/xarray/conventions.py:272\u001b[0m, in \u001b[0;36mdecode_cf_variable\u001b[0;34m(name, var, concat_characters, mask_and_scale, decode_times, decode_endianness, stack_char_dim, use_cftime, decode_timedelta)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m coder \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m    268\u001b[0m         variables\u001b[38;5;241m.\u001b[39mUnsignedIntegerCoder(),\n\u001b[1;32m    269\u001b[0m         variables\u001b[38;5;241m.\u001b[39mCFMaskCoder(),\n\u001b[1;32m    270\u001b[0m         variables\u001b[38;5;241m.\u001b[39mCFScaleOffsetCoder(),\n\u001b[1;32m    271\u001b[0m     ]:\n\u001b[0;32m--> 272\u001b[0m         var \u001b[38;5;241m=\u001b[39m \u001b[43mcoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decode_timedelta:\n",
      "File \u001b[0;32m~/miniconda3/envs/kera_lgbm/lib/python3.9/site-packages/xarray/coding/variables.py:262\u001b[0m, in \u001b[0;36mCFMaskCoder.decode\u001b[0;34m(self, variable, name)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raw_fill_values:\n\u001b[0;32m--> 262\u001b[0m     encoded_fill_values \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    263\u001b[0m         fv\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m option \u001b[38;5;129;01min\u001b[39;00m raw_fill_values\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m fv \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39mravel(option)\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misnull(fv)\n\u001b[1;32m    267\u001b[0m     }\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoded_fill_values) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/kera_lgbm/lib/python3.9/site-packages/xarray/coding/variables.py:265\u001b[0m, in \u001b[0;36m<setcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raw_fill_values:\n\u001b[1;32m    262\u001b[0m     encoded_fill_values \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    263\u001b[0m         fv\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m option \u001b[38;5;129;01min\u001b[39;00m raw_fill_values\n\u001b[0;32m--> 265\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m fv \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43moption\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misnull(fv)\n\u001b[1;32m    267\u001b[0m     }\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoded_fill_values) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mravel\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/kera_lgbm/lib/python3.9/site-packages/numpy/core/fromnumeric.py:1882\u001b[0m, in \u001b[0;36mravel\u001b[0;34m(a, order)\u001b[0m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a contiguous flattened array.\u001b[39;00m\n\u001b[1;32m   1784\u001b[0m \n\u001b[1;32m   1785\u001b[0m \u001b[38;5;124;03mA 1-D array, containing the elements of the input, is returned.  A copy is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1880\u001b[0m \n\u001b[1;32m   1881\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1882\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatrix\u001b[49m):\n\u001b[1;32m   1883\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m asarray(a)\u001b[38;5;241m.\u001b[39mravel(order\u001b[38;5;241m=\u001b[39morder)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'matrix'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m open_and_concatenate(\u001b[38;5;28mstr\u001b[39m(year), variable, month_next, way, level)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 19\u001b[0m     dataset_act \u001b[38;5;241m=\u001b[39m \u001b[43mopen_and_concatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonth_act\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     dataset_next \u001b[38;5;241m=\u001b[39m open_and_concatenate(\u001b[38;5;28mstr\u001b[39m(year_next), variable, month_next, way, level)\n\u001b[1;32m     21\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mconcat([dataset_act, dataset_next], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[20], line 18\u001b[0m, in \u001b[0;36mopen_and_concatenate\u001b[0;34m(year, variable, months, way, level)\u001b[0m\n\u001b[1;32m     16\u001b[0m datasets \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m month \u001b[38;5;129;01min\u001b[39;00m months:\n\u001b[0;32m---> 18\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mway\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mvariable\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/ERA5_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43myear\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmonth\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mvariable\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.nc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m variable \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeopotential\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m level \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     20\u001b[0m         dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39msel(level\u001b[38;5;241m=\u001b[39mlevel)\n",
      "File \u001b[0;32m~/miniconda3/envs/kera_lgbm/lib/python3.9/site-packages/xarray/backends/api.py:525\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    513\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    514\u001b[0m     decode_cf,\n\u001b[1;32m    515\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    521\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    522\u001b[0m )\n\u001b[1;32m    524\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 525\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    532\u001b[0m     backend_ds,\n\u001b[1;32m    533\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    542\u001b[0m )\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/miniconda3/envs/kera_lgbm/lib/python3.9/site-packages/xarray/backends/netCDF4_.py:602\u001b[0m, in \u001b[0;36mNetCDF4BackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, lock, autoclose)\u001b[0m\n\u001b[1;32m    600\u001b[0m store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n\u001b[0;32m--> 602\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[43mstore_entrypoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_and_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_and_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconcat_characters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcat_characters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_coords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_coords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cftime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cftime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_timedelta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_timedelta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/miniconda3/envs/kera_lgbm/lib/python3.9/site-packages/xarray/backends/store.py:46\u001b[0m, in \u001b[0;36mStoreBackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mvars\u001b[39m, attrs \u001b[38;5;241m=\u001b[39m filename_or_obj\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m     44\u001b[0m encoding \u001b[38;5;241m=\u001b[39m filename_or_obj\u001b[38;5;241m.\u001b[39mget_encoding()\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28mvars\u001b[39m, attrs, coord_names \u001b[38;5;241m=\u001b[39m \u001b[43mconventions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_cf_variables\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mvars\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_and_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_and_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcat_characters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcat_characters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_coords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_coords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cftime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cftime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_timedelta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_timedelta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m ds \u001b[38;5;241m=\u001b[39m Dataset(\u001b[38;5;28mvars\u001b[39m, attrs\u001b[38;5;241m=\u001b[39mattrs)\n\u001b[1;32m     59\u001b[0m ds \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mset_coords(coord_names\u001b[38;5;241m.\u001b[39mintersection(\u001b[38;5;28mvars\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/kera_lgbm/lib/python3.9/site-packages/xarray/conventions.py:438\u001b[0m, in \u001b[0;36mdecode_cf_variables\u001b[0;34m(variables, attributes, concat_characters, mask_and_scale, decode_times, decode_coords, drop_variables, use_cftime, decode_timedelta)\u001b[0m\n\u001b[1;32m    427\u001b[0m     new_vars[k] \u001b[38;5;241m=\u001b[39m decode_cf_variable(\n\u001b[1;32m    428\u001b[0m         k,\n\u001b[1;32m    429\u001b[0m         v,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    435\u001b[0m         decode_timedelta\u001b[38;5;241m=\u001b[39mdecode_timedelta,\n\u001b[1;32m    436\u001b[0m     )\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 438\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to decode variable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decode_coords \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoordinates\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    440\u001b[0m     var_attrs \u001b[38;5;241m=\u001b[39m new_vars[k]\u001b[38;5;241m.\u001b[39mattrs\n",
      "\u001b[0;31mAttributeError\u001b[0m: Failed to decode variable '2t': module 'numpy' has no attribute 'matrix'"
     ]
    }
   ],
   "source": [
    "year = int(year)\n",
    "year_next = year + 1\n",
    "month_act = [10, 11, 12]\n",
    "month_next = [1, 2, 3]\n",
    "if variable == 'geopotential':\n",
    "    way = '/work/FAC/FGSE/IDYST/tbeucler/default/raw_data/ECMWF/ERA5_hourly_PL/'\n",
    "else:\n",
    "    way = '/work/FAC/FGSE/IDYST/tbeucler/default/raw_data/ECMWF/ERA5/SL/'\n",
    "\n",
    "# Open and concatenate datasets\n",
    "if year == 1990:\n",
    "    dataset_act = open_and_concatenate(str(year), variable, month_next, way, level)\n",
    "    dataset_next = open_and_concatenate(str(year_next), variable, month_next, way, level)\n",
    "    dataset = xr.concat([dataset_act, dataset_next], dim='time')\n",
    "    dataset = dataset.chunk({'time': 10})\n",
    "elif year == 2021:\n",
    "    dataset = open_and_concatenate(str(year), variable, month_next, way, level)\n",
    "else:\n",
    "    dataset_act = open_and_concatenate(str(year), variable, month_act, way, level)\n",
    "    dataset_next = open_and_concatenate(str(year_next), variable, month_next, way, level)\n",
    "    dataset = xr.concat([dataset_act, dataset_next], dim='time')\n",
    "    dataset = dataset.chunk({'time': 10})\n",
    "\n",
    "# Determine the specific variable to extract\n",
    "specific_var = next(var for var in dataset.variables if var not in ['longitude', 'latitude', 'time', 'level'])\n",
    "\n",
    "# Import all tracks and convert dates\n",
    "dates = pd.read_csv(f'/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/curnagl/updated_storms_with_steps_and_index_ordered.csv', \n",
    "                    parse_dates=['start_date', 'end_date'])\n",
    "dates['year'] = dates['start_date'].dt.year\n",
    "dates.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "# Find the indices for storms within the specified timeframe\n",
    "if year == 1990:\n",
    "    index_start_october = dates[(dates['start_date'].dt.month <= 3) & (dates['start_date'].dt.year == year)].index[0]\n",
    "    index_end_march = dates[(dates['end_date'].dt.month <= 3) & (dates['end_date'].dt.year == year_next)].index[0]\n",
    "elif year == 2021:\n",
    "    index_start_october = dates[(dates['start_date'].dt.month <= 3) & (dates['start_date'].dt.year == year)].index[0]\n",
    "    index_end_march = dates[(dates['end_date'].dt.year == 2021)].index[0]\n",
    "else:\n",
    "# Chercher start_october dans year, sinon chercher dès janvier de year_next\n",
    "    index_start_october = dates[((dates['start_date'].dt.month >= 10) & (dates['start_date'].dt.year == year)) | ((dates['start_date'].dt.year == year_next) & (dates['start_date'].dt.month >= 1))].index[0]\n",
    "    index_end_march_first = dates[((dates['end_date'].dt.month <= 3) & (dates['end_date'].dt.year == year_next))].index\n",
    "    #print(index_start_october, index_end_march_first, '3rd condition start_october + index_end_march_first')\n",
    "    if len(index_end_march_first) > 0:\n",
    "        index_end_march = index_end_march_first[-1]\n",
    "        #print(index_end_march, 'index_end_march 1st condition of 2nd condition')\n",
    "    else:\n",
    "        # Si year_next ne renvoie rien, chercher la dernière instance de tempête dans year\n",
    "        index_end_march = dates[((dates['end_date'].dt.year == year) & (dates['end_date'].dt.month <= 12))].index[-1]\n",
    "        #print(index_end_march, 'index_end_march 2nd condition of 2nd condition')\n",
    "# Process each storm\n",
    "for i in range(index_start_october, index_end_march + 1):\n",
    "    storm_number = dates.at[i, 'storm_index']\n",
    "    track = pd.read_csv(f'{track_path}/storm_{storm_number}.csv')\n",
    "    start_date = dates.at[i, 'start_date']\n",
    "    end_date = dates.at[i, 'end_date']\n",
    "    storm_data = dataset[specific_var].sel(time=slice(start_date, end_date))\n",
    "\n",
    "    # Initialize lists to store statistics\n",
    "    stats = {'mean': [], 'min': [], 'max': [], 'std': []}\n",
    "    #, 'skewness': [], 'kurtosis': []\n",
    "    \n",
    "    # Process each time step\n",
    "    for t_index in range(0, len(storm_data.time)):#, time_step in enumerate(storm_data.time):\n",
    "        #data_slice = storm_data.sel(time=time_step).values\n",
    "\n",
    "        # Extract coordinates for the current time step\n",
    "        lon_e_temp = track.loc[t_index, 'lon_east']\n",
    "        lon_w_temp = track.loc[t_index, 'lon_west']\n",
    "        lat_s_temp = track.loc[t_index, 'lat_south']\n",
    "        lat_n_temp = track.loc[t_index, 'lat_north']\n",
    "        \n",
    "        lon_test = np.asanyarray(storm_data.longitude[:])\n",
    "        lat_test = np.asanyarray(storm_data.latitude[:])\n",
    "\n",
    "        closest_lon_w = np.abs(lon_test - lon_w_temp).argmin()\n",
    "        closest_lon_e = np.abs(lon_test - lon_e_temp).argmin()\n",
    "        closest_lat_s = np.abs(lat_test - lat_s_temp).argmin()\n",
    "        closest_lat_n = np.abs(lat_test - lat_n_temp).argmin()\n",
    "\n",
    "        closest_lon_w_coor = lon_test[closest_lon_w]\n",
    "        closest_lon_e_coor = lon_test[closest_lon_e]\n",
    "        closest_lat_s_coor = lat_test[closest_lat_s]\n",
    "        closest_lat_n_coor = lat_test[closest_lat_n]\n",
    "\n",
    "        # Use .roll to handle the 0°/360° boundary\n",
    "        if closest_lon_w_coor < 100 and closest_lon_e_coor > 100:\n",
    "            roll_shift = {'longitude': int(round(closest_lon_w_coor, 0)), 'longitude': int(round(closest_lon_e_coor, 0))}\n",
    "            storm_data_rolled = storm_data.roll(roll_shift, roll_coords=True)\n",
    "        else:\n",
    "            storm_data_rolled = storm_data\n",
    "\n",
    "        # Slice the dataset based on the rolled longitudes and latitudes\n",
    "        temp_ds_time = storm_data_rolled.isel(time=t_index)#[specific_var].isel(time=t_index)\n",
    "        temp_ds = temp_ds_time.sel(latitude=slice(closest_lat_n_coor, closest_lat_s_coor),\n",
    "                                    longitude=slice(closest_lon_e_coor, closest_lon_w_coor)).values\n",
    "\n",
    "        # Calculate statistics for the sliced data\n",
    "        step_stats = calculate_statistics(temp_ds)\n",
    "        for key in stats:\n",
    "            stats[key].append(step_stats[key])\n",
    "\n",
    "\n",
    "    # Save statistics to CSV files\n",
    "    for key in stats:\n",
    "        directory = f'{dataset_path}/{variable}/storm_{storm_number}'\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        pd.DataFrame(stats[key]).to_csv(f'{directory}/{key}_{storm_number}_{level}.csv')\n",
    "\n",
    "    # Log the processing details\n",
    "    log_processing(variable, year, level, storm_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body[data-theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-index-preview {\n",
       "  grid-column: 2 / 5;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data,\n",
       ".xr-index-data-in:checked ~ .xr-index-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-index-name div,\n",
       ".xr-index-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2,\n",
       ".xr-no-icon {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.DataArray &#x27;2t&#x27; (time: 516, latitude: 721, longitude: 1440)&gt;\n",
       "dask.array&lt;getitem, shape=(516, 721, 1440), dtype=float32, chunksize=(10, 721, 1440), chunktype=numpy.ndarray&gt;\n",
       "Coordinates:\n",
       "  * latitude   (latitude) float64 90.0 89.75 89.5 89.25 ... -89.5 -89.75 -90.0\n",
       "  * longitude  (longitude) float64 0.0 0.25 0.5 0.75 ... 359.0 359.2 359.5 359.8\n",
       "  * time       (time) datetime64[ns] 2013-10-12T06:00:00 ... 2013-12-15T15:00:00\n",
       "Attributes: (12/14)\n",
       "    long_name:                     2 metre temperature\n",
       "    short_name:                    2t\n",
       "    units:                         K\n",
       "    original_format:               WMO GRIB 1 with ECMWF local table\n",
       "    ecmwf_local_table:             128\n",
       "    ecmwf_parameter:               167\n",
       "    ...                            ...\n",
       "    grid_specification:            0.25 degree x 0.25 degree from 90N to 90S ...\n",
       "    rda_dataset:                   ds633.0\n",
       "    rda_dataset_url:               https:/rda.ucar.edu/datasets/ds633.0/\n",
       "    rda_dataset_doi:               DOI: 10.5065/BH6N-5N20\n",
       "    rda_dataset_group:             ERA5 atmospheric surface analysis [netCDF4]\n",
       "    number_of_significant_digits:  7</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.DataArray</div><div class='xr-array-name'>'2t'</div><ul class='xr-dim-list'><li><span class='xr-has-index'>time</span>: 516</li><li><span class='xr-has-index'>latitude</span>: 721</li><li><span class='xr-has-index'>longitude</span>: 1440</li></ul></div><ul class='xr-sections'><li class='xr-section-item'><div class='xr-array-wrap'><input id='section-55d23d05-4f36-44cc-aff9-bcf0902ebd38' class='xr-array-in' type='checkbox' checked><label for='section-55d23d05-4f36-44cc-aff9-bcf0902ebd38' title='Show/hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-array-preview xr-preview'><span>dask.array&lt;chunksize=(10, 721, 1440), meta=np.ndarray&gt;</span></div><div class='xr-array-data'><table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table style=\"border-collapse: collapse;\">\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 2.00 GiB </td>\n",
       "                        <td> 39.61 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (516, 721, 1440) </td>\n",
       "                        <td> (10, 721, 1440) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Dask graph </th>\n",
       "                        <td colspan=\"2\"> 52 chunks in 2 graph layers </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Data type </th>\n",
       "                        <td colspan=\"2\"> float32 numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"205\" height=\"136\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"35\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"10\" y1=\"60\" x2=\"35\" y2=\"86\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"10\" y2=\"60\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"11\" y1=\"1\" x2=\"11\" y2=\"61\" />\n",
       "  <line x1=\"12\" y1=\"2\" x2=\"12\" y2=\"62\" />\n",
       "  <line x1=\"14\" y1=\"4\" x2=\"14\" y2=\"64\" />\n",
       "  <line x1=\"15\" y1=\"5\" x2=\"15\" y2=\"65\" />\n",
       "  <line x1=\"16\" y1=\"6\" x2=\"16\" y2=\"66\" />\n",
       "  <line x1=\"18\" y1=\"8\" x2=\"18\" y2=\"68\" />\n",
       "  <line x1=\"19\" y1=\"9\" x2=\"19\" y2=\"69\" />\n",
       "  <line x1=\"20\" y1=\"10\" x2=\"20\" y2=\"70\" />\n",
       "  <line x1=\"22\" y1=\"12\" x2=\"22\" y2=\"72\" />\n",
       "  <line x1=\"23\" y1=\"13\" x2=\"23\" y2=\"73\" />\n",
       "  <line x1=\"25\" y1=\"15\" x2=\"25\" y2=\"75\" />\n",
       "  <line x1=\"26\" y1=\"16\" x2=\"26\" y2=\"76\" />\n",
       "  <line x1=\"27\" y1=\"17\" x2=\"27\" y2=\"77\" />\n",
       "  <line x1=\"29\" y1=\"19\" x2=\"29\" y2=\"79\" />\n",
       "  <line x1=\"30\" y1=\"20\" x2=\"30\" y2=\"80\" />\n",
       "  <line x1=\"31\" y1=\"21\" x2=\"31\" y2=\"81\" />\n",
       "  <line x1=\"33\" y1=\"23\" x2=\"33\" y2=\"83\" />\n",
       "  <line x1=\"34\" y1=\"24\" x2=\"34\" y2=\"84\" />\n",
       "  <line x1=\"35\" y1=\"25\" x2=\"35\" y2=\"86\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.0,0.0 35.945039876800635,25.94503987680063 35.945039876800635,86.02837321013396 10.0,60.083333333333336\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"130\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"11\" y1=\"1\" x2=\"131\" y2=\"1\" />\n",
       "  <line x1=\"12\" y1=\"2\" x2=\"132\" y2=\"2\" />\n",
       "  <line x1=\"14\" y1=\"4\" x2=\"134\" y2=\"4\" />\n",
       "  <line x1=\"15\" y1=\"5\" x2=\"135\" y2=\"5\" />\n",
       "  <line x1=\"16\" y1=\"6\" x2=\"136\" y2=\"6\" />\n",
       "  <line x1=\"18\" y1=\"8\" x2=\"138\" y2=\"8\" />\n",
       "  <line x1=\"19\" y1=\"9\" x2=\"139\" y2=\"9\" />\n",
       "  <line x1=\"20\" y1=\"10\" x2=\"140\" y2=\"10\" />\n",
       "  <line x1=\"22\" y1=\"12\" x2=\"142\" y2=\"12\" />\n",
       "  <line x1=\"23\" y1=\"13\" x2=\"143\" y2=\"13\" />\n",
       "  <line x1=\"25\" y1=\"15\" x2=\"145\" y2=\"15\" />\n",
       "  <line x1=\"26\" y1=\"16\" x2=\"146\" y2=\"16\" />\n",
       "  <line x1=\"27\" y1=\"17\" x2=\"147\" y2=\"17\" />\n",
       "  <line x1=\"29\" y1=\"19\" x2=\"149\" y2=\"19\" />\n",
       "  <line x1=\"30\" y1=\"20\" x2=\"150\" y2=\"20\" />\n",
       "  <line x1=\"31\" y1=\"21\" x2=\"151\" y2=\"21\" />\n",
       "  <line x1=\"33\" y1=\"23\" x2=\"153\" y2=\"23\" />\n",
       "  <line x1=\"34\" y1=\"24\" x2=\"154\" y2=\"24\" />\n",
       "  <line x1=\"35\" y1=\"25\" x2=\"155\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"35\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"130\" y1=\"0\" x2=\"155\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.0,0.0 130.0,0.0 155.94503987680062,25.94503987680063 35.945039876800635,25.94503987680063\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"35\" y1=\"25\" x2=\"155\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"35\" y1=\"86\" x2=\"155\" y2=\"86\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"35\" y1=\"25\" x2=\"35\" y2=\"86\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"155\" y1=\"25\" x2=\"155\" y2=\"86\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"35.945039876800635,25.94503987680063 155.94503987680065,25.94503987680063 155.94503987680065,86.02837321013396 35.945039876800635,86.02837321013396\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"95.945040\" y=\"106.028373\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >1440</text>\n",
       "  <text x=\"175.945040\" y=\"55.986707\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,175.945040,55.986707)\">721</text>\n",
       "  <text x=\"12.972520\" y=\"93.055853\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(45,12.972520,93.055853)\">516</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table></div></div></li><li class='xr-section-item'><input id='section-829d6809-5916-4927-9106-0c601545a761' class='xr-section-summary-in' type='checkbox'  checked><label for='section-829d6809-5916-4927-9106-0c601545a761' class='xr-section-summary' >Coordinates: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>latitude</span></div><div class='xr-var-dims'>(latitude)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>90.0 89.75 89.5 ... -89.75 -90.0</div><input id='attrs-7223c32e-de23-4707-9822-124c1525d50e' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-7223c32e-de23-4707-9822-124c1525d50e' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-b03724ed-2ce7-42d7-a31b-3e0fbf5d507d' class='xr-var-data-in' type='checkbox'><label for='data-b03724ed-2ce7-42d7-a31b-3e0fbf5d507d' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>latitude</dd><dt><span>short_name :</span></dt><dd>lat</dd><dt><span>units :</span></dt><dd>degrees_north</dd><dt><span>qualifier :</span></dt><dd>Gaussian</dd></dl></div><div class='xr-var-data'><pre>array([ 90.  ,  89.75,  89.5 , ..., -89.5 , -89.75, -90.  ])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>longitude</span></div><div class='xr-var-dims'>(longitude)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>0.0 0.25 0.5 ... 359.2 359.5 359.8</div><input id='attrs-84ae211f-7218-4a95-b21f-191c0bcfb5fd' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-84ae211f-7218-4a95-b21f-191c0bcfb5fd' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-ca8ef0b7-26c5-4ae4-93ed-cf62e44228b1' class='xr-var-data-in' type='checkbox'><label for='data-ca8ef0b7-26c5-4ae4-93ed-cf62e44228b1' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>longitude</dd><dt><span>short_name :</span></dt><dd>lon</dd><dt><span>units :</span></dt><dd>degrees_east</dd></dl></div><div class='xr-var-data'><pre>array([0.0000e+00, 2.5000e-01, 5.0000e-01, ..., 3.5925e+02, 3.5950e+02,\n",
       "       3.5975e+02])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>time</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>2013-10-12T06:00:00 ... 2013-12-...</div><input id='attrs-3e04a043-729f-4758-8ad2-c43dd52b5601' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-3e04a043-729f-4758-8ad2-c43dd52b5601' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-1c41aa5a-22f2-4816-9edd-e3bfd0031746' class='xr-var-data-in' type='checkbox'><label for='data-1c41aa5a-22f2-4816-9edd-e3bfd0031746' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>time</dd></dl></div><div class='xr-var-data'><pre>array([&#x27;2013-10-12T06:00:00.000000000&#x27;, &#x27;2013-10-12T09:00:00.000000000&#x27;,\n",
       "       &#x27;2013-10-12T12:00:00.000000000&#x27;, ..., &#x27;2013-12-15T09:00:00.000000000&#x27;,\n",
       "       &#x27;2013-12-15T12:00:00.000000000&#x27;, &#x27;2013-12-15T15:00:00.000000000&#x27;],\n",
       "      dtype=&#x27;datetime64[ns]&#x27;)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-0c03bf0b-7c9e-4b79-815f-fb82d0cf4c85' class='xr-section-summary-in' type='checkbox'  ><label for='section-0c03bf0b-7c9e-4b79-815f-fb82d0cf4c85' class='xr-section-summary' >Indexes: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-index-name'><div>latitude</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-2b5fb444-5a1c-45e3-b9f7-832dbc393bea' class='xr-index-data-in' type='checkbox'/><label for='index-2b5fb444-5a1c-45e3-b9f7-832dbc393bea' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([  90.0,  89.75,   89.5,  89.25,   89.0,  88.75,   88.5,  88.25,   88.0,\n",
       "        87.75,\n",
       "       ...\n",
       "       -87.75,  -88.0, -88.25,  -88.5, -88.75,  -89.0, -89.25,  -89.5, -89.75,\n",
       "        -90.0],\n",
       "      dtype=&#x27;float64&#x27;, name=&#x27;latitude&#x27;, length=721))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>longitude</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-c08daa7d-c6e0-4d22-ba8e-d31ad28968d3' class='xr-index-data-in' type='checkbox'/><label for='index-c08daa7d-c6e0-4d22-ba8e-d31ad28968d3' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([   0.0,   0.25,    0.5,   0.75,    1.0,   1.25,    1.5,   1.75,    2.0,\n",
       "         2.25,\n",
       "       ...\n",
       "        357.5, 357.75,  358.0, 358.25,  358.5, 358.75,  359.0, 359.25,  359.5,\n",
       "       359.75],\n",
       "      dtype=&#x27;float64&#x27;, name=&#x27;longitude&#x27;, length=1440))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>time</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-6d10c6e3-453c-4813-b0a3-4c99e923089d' class='xr-index-data-in' type='checkbox'/><label for='index-6d10c6e3-453c-4813-b0a3-4c99e923089d' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(DatetimeIndex([&#x27;2013-10-12 06:00:00&#x27;, &#x27;2013-10-12 09:00:00&#x27;,\n",
       "               &#x27;2013-10-12 12:00:00&#x27;, &#x27;2013-10-12 15:00:00&#x27;,\n",
       "               &#x27;2013-10-12 18:00:00&#x27;, &#x27;2013-10-12 21:00:00&#x27;,\n",
       "               &#x27;2013-10-13 00:00:00&#x27;, &#x27;2013-10-13 03:00:00&#x27;,\n",
       "               &#x27;2013-10-13 06:00:00&#x27;, &#x27;2013-10-13 09:00:00&#x27;,\n",
       "               ...\n",
       "               &#x27;2013-12-14 12:00:00&#x27;, &#x27;2013-12-14 15:00:00&#x27;,\n",
       "               &#x27;2013-12-14 18:00:00&#x27;, &#x27;2013-12-14 21:00:00&#x27;,\n",
       "               &#x27;2013-12-15 00:00:00&#x27;, &#x27;2013-12-15 03:00:00&#x27;,\n",
       "               &#x27;2013-12-15 06:00:00&#x27;, &#x27;2013-12-15 09:00:00&#x27;,\n",
       "               &#x27;2013-12-15 12:00:00&#x27;, &#x27;2013-12-15 15:00:00&#x27;],\n",
       "              dtype=&#x27;datetime64[ns]&#x27;, name=&#x27;time&#x27;, length=516, freq=None))</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-dade1ba2-35ae-49ea-be1f-bc45285637b5' class='xr-section-summary-in' type='checkbox'  ><label for='section-dade1ba2-35ae-49ea-be1f-bc45285637b5' class='xr-section-summary' >Attributes: <span>(14)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>2 metre temperature</dd><dt><span>short_name :</span></dt><dd>2t</dd><dt><span>units :</span></dt><dd>K</dd><dt><span>original_format :</span></dt><dd>WMO GRIB 1 with ECMWF local table</dd><dt><span>ecmwf_local_table :</span></dt><dd>128</dd><dt><span>ecmwf_parameter :</span></dt><dd>167</dd><dt><span>minimum_value :</span></dt><dd>205.85832</dd><dt><span>maximum_value :</span></dt><dd>318.10947</dd><dt><span>grid_specification :</span></dt><dd>0.25 degree x 0.25 degree from 90N to 90S and 0E to 359.75E (721 x 1440 Latitude/Longitude)</dd><dt><span>rda_dataset :</span></dt><dd>ds633.0</dd><dt><span>rda_dataset_url :</span></dt><dd>https:/rda.ucar.edu/datasets/ds633.0/</dd><dt><span>rda_dataset_doi :</span></dt><dd>DOI: 10.5065/BH6N-5N20</dd><dt><span>rda_dataset_group :</span></dt><dd>ERA5 atmospheric surface analysis [netCDF4]</dd><dt><span>number_of_significant_digits :</span></dt><dd>7</dd></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.DataArray '2t' (time: 516, latitude: 721, longitude: 1440)>\n",
       "dask.array<getitem, shape=(516, 721, 1440), dtype=float32, chunksize=(10, 721, 1440), chunktype=numpy.ndarray>\n",
       "Coordinates:\n",
       "  * latitude   (latitude) float64 90.0 89.75 89.5 89.25 ... -89.5 -89.75 -90.0\n",
       "  * longitude  (longitude) float64 0.0 0.25 0.5 0.75 ... 359.0 359.2 359.5 359.8\n",
       "  * time       (time) datetime64[ns] 2013-10-12T06:00:00 ... 2013-12-15T15:00:00\n",
       "Attributes: (12/14)\n",
       "    long_name:                     2 metre temperature\n",
       "    short_name:                    2t\n",
       "    units:                         K\n",
       "    original_format:               WMO GRIB 1 with ECMWF local table\n",
       "    ecmwf_local_table:             128\n",
       "    ecmwf_parameter:               167\n",
       "    ...                            ...\n",
       "    grid_specification:            0.25 degree x 0.25 degree from 90N to 90S ...\n",
       "    rda_dataset:                   ds633.0\n",
       "    rda_dataset_url:               https:/rda.ucar.edu/datasets/ds633.0/\n",
       "    rda_dataset_doi:               DOI: 10.5065/BH6N-5N20\n",
       "    rda_dataset_group:             ERA5 atmospheric surface analysis [netCDF4]\n",
       "    number_of_significant_digits:  7"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import all tracks and convert dates\n",
    "storm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'year' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m dates[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m dates[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_date\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39myear\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Find the indices for storms within the specified timeframe\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43myear\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1990\u001b[39m:\n\u001b[1;32m      9\u001b[0m     index_start_october \u001b[38;5;241m=\u001b[39m dates[(dates[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_date\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mmonth \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m) \u001b[38;5;241m&\u001b[39m (dates[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_date\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39myear \u001b[38;5;241m==\u001b[39m year)]\u001b[38;5;241m.\u001b[39mindex[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     10\u001b[0m     index_end_march \u001b[38;5;241m=\u001b[39m dates[(dates[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend_date\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mmonth \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m) \u001b[38;5;241m&\u001b[39m (dates[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend_date\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39myear \u001b[38;5;241m==\u001b[39m year_next)]\u001b[38;5;241m.\u001b[39mindex[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'year' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "'''# Import all tracks and convert dates\n",
    "dates = pd.read_csv(f'/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/curnagl/storms_start_end.csv', parse_dates=['start_date', 'end_date'])\n",
    "dates['year'] = dates['start_date'].dt.year\n",
    "\n",
    "# Find the indices for storms within the specified timeframe\n",
    "if year == 1990:\n",
    "    index_start_october = dates[(dates['start_date'].dt.month <= 3) & (dates['start_date'].dt.year == year)].index[0]\n",
    "    index_end_march = dates[(dates['end_date'].dt.month <= 3) & (dates['end_date'].dt.year == year_next)].index[0]\n",
    "elif year == 2021:\n",
    "    index_start_october = dates[(dates['start_date'].dt.month <= 3) & (dates['start_date'].dt.year == year)].index[0]\n",
    "    index_end_march = dates[(dates['end_date'].dt.year == 2021)].index[0]\n",
    "else:'''\n",
    "# Chercher start_october dans year, sinon chercher dès janvier de year_next\n",
    "    index_start_october = dates[((dates['start_date'].dt.month >= 10) & (dates['start_date'].dt.year == year)) | ((dates['start_date'].dt.year == year_next) & (dates['start_date'].dt.month >= 1))].index[0]\n",
    "    index_end_march_first = dates[((dates['end_date'].dt.month <= 3) & (dates['end_date'].dt.year == year_next))].index\n",
    "    #print(index_start_october, index_end_march_first, '3rd condition start_october + index_end_march_first')\n",
    "    if len(index_end_march_first) > 0:\n",
    "        index_end_march = index_end_march_first[-1]\n",
    "        #print(index_end_march, 'index_end_march 1st condition of 2nd condition')\n",
    "    else:\n",
    "        # Si year_next ne renvoie rien, chercher la dernière instance de tempête dans year\n",
    "        index_end_march = dates[((dates['end_date'].dt.year == year) & (dates['end_date'].dt.month <= 12))].index[-1]\n",
    "        #print(index_end_march, 'index_end_march 2nd condition of 2nd condition')\n",
    "#index_start_october = pd.read_csv(f'/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/curnagl/ALL_TRACKS/just_in_case/storm_{i+1}.csv')\n",
    "# Process each storm\n",
    "for i in range(1,97):#index_start_october, index_end_march + 1):\n",
    "    dates = pd.read_csv(f'/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/curnagl/ALL_TRACKS/just_in_case/storm_{i+1}.csv')\n",
    "    dates = dates['Date']\n",
    "    year = int(dates[:4])\n",
    "    month = int(dates[4:6])\n",
    "    day = int(dates[6:8])\n",
    "    hour = int(dates[8:10])\n",
    "\n",
    "    # Create a datetime object\n",
    "    dt = datetime(year, month, day, hour)\n",
    "\n",
    "    # Format the datetime object into the desired string format\n",
    "    formatted_date = dt.strftime(\"%Y-%m-%d %H:00:00\")\n",
    "    track = pd.read_csv(f'{track_path}/storm_{i+1}.csv')\n",
    "    start_date = dates.at[i, 'start_date']\n",
    "    end_date = dates.at[i, 'end_date']\n",
    "    storm_data = dataset[specific_var].sel(time=slice(start_date, end_date))\n",
    "\n",
    "    # Initialize lists to store statistics\n",
    "    stats = {'mean': [], 'min': [], 'max': [], 'std': []}\n",
    "    #, 'skewness': [], 'kurtosis': []\n",
    "    \n",
    "    # Process each time step\n",
    "    for t_index in range(0, len(storm_data.time)):#, time_step in enumerate(storm_data.time):\n",
    "        #data_slice = storm_data.sel(time=time_step).values\n",
    "\n",
    "        # Extract coordinates for the current time step\n",
    "        lon_e_temp = track.loc[t_index, 'lon_east']\n",
    "        lon_w_temp = track.loc[t_index, 'lon_west']\n",
    "        lat_s_temp = track.loc[t_index, 'lat_south']\n",
    "        lat_n_temp = track.loc[t_index, 'lat_north']\n",
    "        \n",
    "        lon_test = np.asanyarray(storm_data.longitude[:])\n",
    "        lat_test = np.asanyarray(storm_data.latitude[:])\n",
    "\n",
    "        closest_lon_w = np.abs(lon_test - lon_w_temp).argmin()\n",
    "        closest_lon_e = np.abs(lon_test - lon_e_temp).argmin()\n",
    "        closest_lat_s = np.abs(lat_test - lat_s_temp).argmin()\n",
    "        closest_lat_n = np.abs(lat_test - lat_n_temp).argmin()\n",
    "\n",
    "        closest_lon_w_coor = lon_test[closest_lon_w]\n",
    "        closest_lon_e_coor = lon_test[closest_lon_e]\n",
    "        closest_lat_s_coor = lat_test[closest_lat_s]\n",
    "        closest_lat_n_coor = lat_test[closest_lat_n]\n",
    "\n",
    "        # Use .roll to handle the 0°/360° boundary\n",
    "        if closest_lon_w_coor < 100 and closest_lon_e_coor > 100:\n",
    "            roll_shift = {'longitude': int(round(closest_lon_w_coor, 0)), 'longitude': int(round(closest_lon_e_coor, 0))}\n",
    "            storm_data_rolled = storm_data.roll(roll_shift, roll_coords=True)\n",
    "        else:\n",
    "            storm_data_rolled = storm_data\n",
    "\n",
    "        # Slice the dataset based on the rolled longitudes and latitudes\n",
    "        temp_ds_time = storm_data_rolled.isel(time=t_index)#[specific_var].isel(time=t_index)\n",
    "        temp_ds = temp_ds_time.sel(latitude=slice(closest_lat_n_coor, closest_lat_s_coor),\n",
    "                                    longitude=slice(closest_lon_e_coor, closest_lon_w_coor)).values\n",
    "\n",
    "        # Calculate statistics for the sliced data\n",
    "        step_stats = calculate_statistics(temp_ds)\n",
    "        for key in stats:\n",
    "            stats[key].append(step_stats[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"if __name__ == '__main__':\\n    variable = sys.argv[1]\\n    year = sys.argv[2]\\n    level = int(sys.argv[3])\\n\\n    if not all_csv_files_exist(variable, year, level):\\n        process_data(variable, year, level)\\n    else:\\n        print(f'All CSV files for variable: {variable}, year: {year}, and level: {level} already exist. Skipping processing.')\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "dataset_path = '/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/curnagl/DATASETS/datasets_3h'\n",
    "track_path = '/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/curnagl/ALL_TRACKS/tracks_3h'\n",
    "\n",
    "# Works for all years\n",
    "# Define a function to open datasets and concatenate them\n",
    "def open_and_concatenate(year, variable, months, way, level=0):\n",
    "    datasets = []\n",
    "    for month in months:\n",
    "        dataset = xr.open_dataset(f'{way}{variable}/ERA5_{year}-{month}_{variable}.nc')\n",
    "        if variable == 'geopotential' and level != 0:\n",
    "            dataset = dataset.sel(level=level)\n",
    "        \n",
    "        # Create a date range with 3-hour intervals starting from midnight\n",
    "        start = pd.Timestamp(f\"{year}-{month}-01 00:00:00\")\n",
    "        if month == 12:\n",
    "            end = pd.date_range(start=f\"{year}-{month}-01\", end=f\"{str(int(year)+1)}-01-01\", freq='M')[0] + pd.Timedelta(hours=21)\n",
    "        else:\n",
    "            end = pd.date_range(start=f\"{year}-{month}-01\", end=f\"{year}-{month+1}-01\", freq='M')[0] + pd.Timedelta(hours=21)\n",
    "        date_range = pd.date_range(start, end, freq='3H')\n",
    "\n",
    "        # Select the data at the specific timesteps\n",
    "        dataset = dataset.sel(time=date_range)\n",
    "        \n",
    "        datasets.append(dataset)\n",
    "        dataset.close()\n",
    "\n",
    "    return xr.concat(datasets, dim='time')\n",
    "\n",
    "# Define a function to calculate statistics\n",
    "def calculate_statistics(data_array):\n",
    "    return {\n",
    "        'mean': np.mean(data_array),\n",
    "        'min': np.min(data_array),\n",
    "        'max': np.max(data_array),\n",
    "        'std': np.std(data_array),\n",
    "    }\n",
    "\n",
    "# Function to log processing details\n",
    "def log_processing(variable, year, level, storm_number):\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    log_message = f'Processed variable: {variable}, Year: {year}, Level: {level}, Timestamp: {timestamp}, Storm number:{storm_number}'\n",
    "    with open(f'{dataset_path}/processing_log_3h.txt', 'a') as log_file:\n",
    "        log_file.write(log_message + '\\n')\n",
    "\n",
    "# Function to check if all CSV files exist\n",
    "def all_csv_files_exist(variable, year, level):\n",
    "    directory = f'{dataset_path}/{variable}'\n",
    "    if not os.path.exists(directory):\n",
    "        return False\n",
    "\n",
    "    for storm_dir in os.listdir(directory):\n",
    "        storm_path = os.path.join(directory, storm_dir)\n",
    "        if os.path.isdir(storm_path):\n",
    "            for stat in ['mean', 'min', 'max', 'std']:\n",
    "                file_path = os.path.join(storm_path, f'{stat}_{storm_dir.split(\"_\")[1]}_{level}.csv')\n",
    "                if not os.path.exists(file_path):\n",
    "                    return False\n",
    "    return True\n",
    "\n",
    "# Main function to process data\n",
    "def process_data(variable, year, level=0):\n",
    "    year = int(year)\n",
    "    year_next = year + 1\n",
    "    month_act = [10, 11, 12]\n",
    "    month_next = [1, 2, 3]\n",
    "    if variable == 'geopotential':\n",
    "        way = '/work/FAC/FGSE/IDYST/tbeucler/default/raw_data/ECMWF/ERA5_hourly_PL/'\n",
    "    else:\n",
    "        way = '/work/FAC/FGSE/IDYST/tbeucler/default/raw_data/ECMWF/ERA5/SL/'\n",
    "\n",
    "    # Open and concatenate datasets\n",
    "    if year == 1990:\n",
    "        dataset_act = open_and_concatenate(str(year), variable, month_next, way, level)\n",
    "        dataset_next = open_and_concatenate(str(year_next), variable, month_next, way, level)\n",
    "        dataset = xr.concat([dataset_act, dataset_next], dim='time')\n",
    "        dataset = dataset.chunk({'time': 10})\n",
    "    elif year == 2021:\n",
    "        dataset = open_and_concatenate(str(year), variable, month_next, way, level)\n",
    "    else:\n",
    "        dataset_act = open_and_concatenate(str(year), variable, month_act, way, level)\n",
    "        dataset_next = open_and_concatenate(str(year_next), variable, month_next, way, level)\n",
    "        dataset = xr.concat([dataset_act, dataset_next], dim='time')\n",
    "        dataset = dataset.chunk({'time': 10})\n",
    "\n",
    "    # Determine the specific variable to extract\n",
    "    specific_var = next(var for var in dataset.variables if var not in ['longitude', 'latitude', 'time', 'level'])\n",
    "\n",
    "    # Import all tracks and convert dates\n",
    "    dates = pd.read_csv(f'/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/curnagl/updated_storms_with_steps_and_index_ordered.csv', \n",
    "                parse_dates=['start_date', 'end_date'])\n",
    "    dates['year'] = dates['start_date'].dt.year\n",
    "    dates.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "    # Find the indices for storms within the specified timeframe\n",
    "    if year == 1990:\n",
    "        index_start_october = dates[(dates['start_date'].dt.month <= 3) & (dates['start_date'].dt.year == year)].index[0]\n",
    "        index_end_march = dates[(dates['end_date'].dt.month <= 3) & (dates['end_date'].dt.year == year_next)].index[0]\n",
    "    elif year == 2021:\n",
    "        index_start_october = dates[(dates['start_date'].dt.month <= 3) & (dates['start_date'].dt.year == year)].index[0]\n",
    "        index_end_march = dates[(dates['end_date'].dt.year == 2021)].index[0]\n",
    "    else:\n",
    "    # Chercher start_october dans year, sinon chercher dès janvier de year_next\n",
    "        index_start_october = dates[((dates['start_date'].dt.month >= 10) & (dates['start_date'].dt.year == year)) | ((dates['start_date'].dt.year == year_next) & (dates['start_date'].dt.month >= 1))].index[0]\n",
    "        index_end_march_first = dates[((dates['end_date'].dt.month <= 3) & (dates['end_date'].dt.year == year_next))].index\n",
    "        #print(index_start_october, index_end_march_first, '3rd condition start_october + index_end_march_first')\n",
    "        if len(index_end_march_first) > 0:\n",
    "            index_end_march = index_end_march_first[-1]\n",
    "            #print(index_end_march, 'index_end_march 1st condition of 2nd condition')\n",
    "        else:\n",
    "            # Si year_next ne renvoie rien, chercher la dernière instance de tempête dans year\n",
    "            index_end_march = dates[((dates['end_date'].dt.year == year) & (dates['end_date'].dt.month <= 12))].index[-1]\n",
    "            #print(index_end_march, 'index_end_march 2nd condition of 2nd condition')\n",
    "    # Process each storm\n",
    "    for i in range(index_start_october, index_end_march + 1):\n",
    "        storm_number = dates.at[i, 'storm_index']\n",
    "        track = pd.read_csv(f'{track_path}/storm_{storm_number}.csv')\n",
    "        start_date = dates.at[i, 'start_date']\n",
    "        end_date = dates.at[i, 'end_date']\n",
    "        storm_data = dataset[specific_var].sel(time=slice(start_date, end_date))\n",
    "\n",
    "        # Initialize lists to store statistics\n",
    "        stats = {'mean': [], 'min': [], 'max': [], 'std': []}\n",
    "        #, 'skewness': [], 'kurtosis': []\n",
    "        \n",
    "        # Process each time step\n",
    "        for t_index in range(0, len(storm_data.time)):#, time_step in enumerate(storm_data.time):\n",
    "            #data_slice = storm_data.sel(time=time_step).values\n",
    "\n",
    "            # Extract coordinates for the current time step\n",
    "\n",
    "            lon_e_temp = track.loc[t_index, 'lon_east']\n",
    "            lon_w_temp = track.loc[t_index, 'lon_west']\n",
    "            lat_s_temp = track.loc[t_index, 'lat_south']\n",
    "            lat_n_temp = track.loc[t_index, 'lat_north']\n",
    "            \n",
    "            lon_test = np.asanyarray(storm_data.longitude[:])\n",
    "            lat_test = np.asanyarray(storm_data.latitude[:])\n",
    "\n",
    "            closest_lon_w = np.abs(lon_test - lon_w_temp).argmin()\n",
    "            closest_lon_e = np.abs(lon_test - lon_e_temp).argmin()\n",
    "            closest_lat_s = np.abs(lat_test - lat_s_temp).argmin()\n",
    "            closest_lat_n = np.abs(lat_test - lat_n_temp).argmin()\n",
    "\n",
    "            closest_lon_w_coor = lon_test[closest_lon_w]\n",
    "            closest_lon_e_coor = lon_test[closest_lon_e]\n",
    "            closest_lat_s_coor = lat_test[closest_lat_s]\n",
    "            closest_lat_n_coor = lat_test[closest_lat_n]\n",
    "\n",
    "            # Use .roll to handle the 0°/360° boundary\n",
    "            if closest_lon_w_coor < 100 and closest_lon_e_coor > 100:\n",
    "                roll_shift = {'longitude': int(round(closest_lon_w_coor, 0)), 'longitude': int(round(closest_lon_e_coor, 0))}\n",
    "                storm_data_rolled = storm_data.roll(roll_shift, roll_coords=True)\n",
    "            else:\n",
    "                storm_data_rolled = storm_data\n",
    "\n",
    "            # Slice the dataset based on the rolled longitudes and latitudes\n",
    "            temp_ds_time = storm_data_rolled.isel(time=t_index)#[specific_var].isel(time=t_index)\n",
    "            temp_ds = temp_ds_time.sel(latitude=slice(closest_lat_n_coor, closest_lat_s_coor),\n",
    "                                       longitude=slice(closest_lon_e_coor, closest_lon_w_coor)).values\n",
    "\n",
    "            # Calculate statistics for the sliced data\n",
    "            step_stats = calculate_statistics(temp_ds)\n",
    "            for key in stats:\n",
    "                stats[key].append(step_stats[key])\n",
    "\n",
    "\n",
    "        # Save statistics to CSV files\n",
    "        for key in stats:\n",
    "            directory = f'{dataset_path}/{variable}/storm_{storm_number}'\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            pd.DataFrame(stats[key]).to_csv(f'{directory}/{key}_{storm_number}_{level}.csv')\n",
    "\n",
    "        # Log the processing details\n",
    "        log_processing(variable, year, level, storm_number)\n",
    "\n",
    "\n",
    "variable = '2m_temperature'\n",
    "year = 2013\n",
    "level = 0\n",
    "process_data(variable, year, level)\n",
    "\n",
    "'''if __name__ == '__main__':\n",
    "    variable = sys.argv[1]\n",
    "    year = sys.argv[2]\n",
    "    level = int(sys.argv[3])\n",
    "\n",
    "    if not all_csv_files_exist(variable, year, level):\n",
    "        process_data(variable, year, level)\n",
    "    else:\n",
    "        print(f'All CSV files for variable: {variable}, year: {year}, and level: {level} already exist. Skipping processing.')'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
