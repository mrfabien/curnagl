{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 138\u001b[0m\n\u001b[1;32m    136\u001b[0m     year \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2006\u001b[39m\n\u001b[1;32m    137\u001b[0m     level \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 138\u001b[0m     \u001b[43mprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# Obtain a snapshot of memory usage\u001b[39;00m\n\u001b[1;32m    141\u001b[0m h \u001b[38;5;241m=\u001b[39m hp\u001b[38;5;241m.\u001b[39mheap()\n",
      "Cell \u001b[0;32mIn[4], line 96\u001b[0m, in \u001b[0;36mprocess_data\u001b[0;34m(variable, year, level)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Process each time step\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t_index, time_step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(storm_data\u001b[38;5;241m.\u001b[39mtime):\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m#data_slice = storm_data.sel(time=time_step).values\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# Extract coordinates for the current time step\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m     lon_e_temp, lon_w_temp, lat_s_temp, lat_n_temp \u001b[38;5;241m=\u001b[39m \u001b[43mtrack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     97\u001b[0m     lon_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(storm_data\u001b[38;5;241m.\u001b[39mlongitude[:])\n\u001b[1;32m     98\u001b[0m     lat_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(storm_data\u001b[38;5;241m.\u001b[39mlatitude[:])\n",
      "File \u001b[0;32m~/miniconda3/envs/kera_lgbm/lib/python3.9/site-packages/pandas/core/indexing.py:1103\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1100\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1102\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m-> 1103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/kera_lgbm/lib/python3.9/site-packages/pandas/core/indexing.py:1656\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index by location index with a non-integer key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1655\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[0;32m-> 1656\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1658\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_ixs(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/miniconda3/envs/kera_lgbm/lib/python3.9/site-packages/pandas/core/indexing.py:1589\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1587\u001b[0m len_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis))\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m len_axis \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mlen_axis:\n\u001b[0;32m-> 1589\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle positional indexer is out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from guppy import hpy\n",
    "\n",
    "# Create an instance of heapy\n",
    "hp = hpy()\n",
    "\n",
    "# Define a function to open datasets and concatenate them\n",
    "def open_and_concatenate(year, variable, months, way, level=0):\n",
    "    datasets = [xr.open_dataset(f'{way}{variable}/ERA5_{year}-{month}_{variable}.nc') for month in months]\n",
    "    if variable == 'geopotential' and level != 0:\n",
    "        datasets = [dataset.sel(level=level) for dataset in datasets]\n",
    "    return xr.concat(datasets, dim='time')\n",
    "\n",
    "# Define a function to calculate statistics\n",
    "def calculate_statistics(data_array):\n",
    "    return {\n",
    "        'mean': np.mean(data_array),\n",
    "        'min': np.min(data_array),\n",
    "        'max': np.max(data_array),\n",
    "        'std': np.std(data_array),\n",
    "    }\n",
    "\n",
    "# Function to log processing details\n",
    "def log_processing(variable, year, level, storm_number):\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    log_message = f'Processed variable: {variable}, Year: {year}, Level: {level}, Timestamp: {timestamp}, Storm number:{storm_number}'\n",
    "    with open(f'/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/curnagl/datasets/processing_log.txt', 'a') as log_file:\n",
    "        log_file.write(log_message + '\\n')\n",
    "\n",
    "# Main function to process data\n",
    "def process_data(variable, year, level=0):\n",
    "    year = int(year)\n",
    "    year_next = year + 1\n",
    "    month_act = [10, 11, 12]\n",
    "    month_next = [1, 2, 3]\n",
    "    if variable == 'geopotential':\n",
    "        way = '/work/FAC/FGSE/IDYST/tbeucler/default/raw_data/ECMWF/ERA5_hourly_PL/'\n",
    "    else:\n",
    "        way = '/work/FAC/FGSE/IDYST/tbeucler/default/raw_data/ECMWF/ERA5/SL/'\n",
    "\n",
    "    # Open and concatenate datasets\n",
    "    if year == 1990:\n",
    "        dataset_act = open_and_concatenate(str(year), variable, month_next, way, level)\n",
    "        dataset_next = open_and_concatenate(str(year_next), variable, month_next, way, level)\n",
    "        dataset = xr.concat([dataset_act, dataset_next], dim='time')\n",
    "        dataset = dataset.chunk({'time': 10})\n",
    "    elif year == 2021:\n",
    "        dataset = open_and_concatenate(str(year), variable, month_next, way, level)\n",
    "    else:\n",
    "        dataset_act = open_and_concatenate(str(year), variable, month_act, way, level)\n",
    "        dataset_next = open_and_concatenate(str(year_next), variable, month_next, way, level)\n",
    "        dataset = xr.concat([dataset_act, dataset_next], dim='time')\n",
    "        dataset = dataset.chunk({'time': 10})\n",
    "\n",
    "    # Determine the specific variable to extract\n",
    "    specific_var = next(var for var in dataset.variables if var not in ['longitude', 'latitude', 'time', 'level'])\n",
    "\n",
    "    # Import all tracks and convert dates\n",
    "    dates = pd.read_csv(f'/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/curnagl/storms_start_end.csv', parse_dates=['start_date', 'end_date'])\n",
    "    dates['year'] = dates['start_date'].dt.year\n",
    "\n",
    "    # Find the indices for storms within the specified timeframe\n",
    "    if year == 1990:\n",
    "        index_start_october = dates[(dates['start_date'].dt.month <= 3) & (dates['start_date'].dt.year == year)].index[0]\n",
    "        index_end_march = dates[(dates['end_date'].dt.month <= 3) & (dates['end_date'].dt.year == year_next)].index[0]\n",
    "    elif year == 2021:\n",
    "        index_start_october = dates[(dates['start_date'].dt.month <= 3) & (dates['start_date'].dt.year == year)].index[0]\n",
    "        index_end_march = dates[(dates['end_date'].dt.year == 2021)].index[0]\n",
    "    else:\n",
    "        index_start_october = dates[((dates['start_date'].dt.month >= 10) & (dates['start_date'].dt.year == year)) | ((dates['start_date'].dt.year == year_next) & (dates['start_date'].dt.month >= 1))].index[0]\n",
    "        index_end_march_first = dates[((dates['end_date'].dt.month <= 3) & (dates['end_date'].dt.year == year_next))].index\n",
    "        if len(index_end_march_first) > 0:\n",
    "            index_end_march = index_end_march_first[-1]\n",
    "        else:\n",
    "            index_end_march = dates[((dates['end_date'].dt.year == year) & (dates['end_date'].dt.month <= 12))].index[-1]\n",
    "\n",
    "    # Process each storm\n",
    "    for i in range(index_start_october, index_end_march + 1):\n",
    "        track = pd.read_csv(f'/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/curnagl/tc_irad_tracks/tc_1_hour/tc_irad_{i+1}_interp.txt')\n",
    "        start_date = dates.at[i, 'start_date']\n",
    "        end_date = dates.at[i, 'end_date']\n",
    "        storm_data = dataset[specific_var].sel(time=slice(start_date, end_date))\n",
    "\n",
    "        # Initialize lists to store statistics\n",
    "        stats = {'mean': [], 'min': [], 'max': [], 'std': []}\n",
    "\n",
    "        # Process each time step\n",
    "        for t_index, time_step in enumerate(storm_data.time):\n",
    "            #data_slice = storm_data.sel(time=time_step).values\n",
    "\n",
    "            # Extract coordinates for the current time step\n",
    "            lon_e_temp, lon_w_temp, lat_s_temp, lat_n_temp = track.iloc[t_index]\n",
    "            lon_test = np.asanyarray(storm_data.longitude[:])\n",
    "            lat_test = np.asanyarray(storm_data.latitude[:])\n",
    "\n",
    "            closest_lon_w = np.abs(lon_test - lon_w_temp).argmin()\n",
    "            closest_lon_e = np.abs(lon_test - lon_e_temp).argmin()\n",
    "            closest_lat_s = np.abs(lat_test - lat_s_temp).argmin()\n",
    "            closest_lat_n = np.abs(lat_test - lat_n_temp).argmin()\n",
    "\n",
    "            closest_lon_w_coor = lon_test[closest_lon_w]\n",
    "            closest_lon_e_coor = lon_test[closest_lon_e]\n",
    "            closest_lat_s_coor = lat_test[closest_lat_s]\n",
    "            closest_lat_n_coor = lat_test[closest_lat_n]\n",
    "\n",
    "            # Use .roll to handle the 0째/360째 boundary\n",
    "            if closest_lon_w_coor < 100 and closest_lon_e_coor > 100:\n",
    "                roll_shift = {'longitude': int(round(closest_lon_w_coor, 0)), 'longitude': int(round(closest_lon_e_coor, 0))}\n",
    "                storm_data_rolled = storm_data.roll(roll_shift, roll_coords=True)\n",
    "            else:\n",
    "                storm_data_rolled = storm_data\n",
    "\n",
    "            # Slice the dataset based on the rolled longitudes and latitudes\n",
    "            temp_ds_time = storm_data_rolled.isel(time=t_index)#[specific_var].isel(time=t_index)\n",
    "            temp_ds = temp_ds_time.sel(latitude=slice(closest_lat_n_coor, closest_lat_s_coor),\n",
    "                                       longitude=slice(closest_lon_e_coor, closest_lon_w_coor)).values\n",
    "\n",
    "            # Calculate statistics for the sliced data\n",
    "            step_stats = calculate_statistics(temp_ds)\n",
    "            for key in stats:\n",
    "                stats[key].append(step_stats[key])\n",
    "\n",
    "        # Save statistics to CSV files\n",
    "        for key in stats:\n",
    "            pd.DataFrame(stats[key]).to_csv(f'/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/curnagl/datasets/{variable}/storm_{i+1}/{key}_{i+1}_{level}.csv')\n",
    "\n",
    "    # Log the processing details\n",
    "    log_processing(variable, year, level, i+1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    variable = '2m_temperature'\n",
    "    year = 2006\n",
    "    level = 0\n",
    "    process_data(variable, year, level)\n",
    "\n",
    "# Obtain a snapshot of memory usage\n",
    "h = hp.heap()\n",
    "\n",
    "# Print memory usage information\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 61\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Process each time step\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t_index, time_step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(storm_data\u001b[38;5;241m.\u001b[39mtime):\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m#data_slice = storm_data.sel(time=time_step).values\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# Extract coordinates for the current time step\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     lon_e_temp, lon_w_temp, lat_s_temp, lat_n_temp \u001b[38;5;241m=\u001b[39m \u001b[43mtrack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     62\u001b[0m     lon_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(storm_data\u001b[38;5;241m.\u001b[39mlongitude[:])\n\u001b[1;32m     63\u001b[0m     lat_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(storm_data\u001b[38;5;241m.\u001b[39mlatitude[:])\n",
      "File \u001b[0;32m~/miniconda3/envs/kera_lgbm/lib/python3.9/site-packages/pandas/core/indexing.py:1103\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1100\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1102\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m-> 1103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/kera_lgbm/lib/python3.9/site-packages/pandas/core/indexing.py:1656\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index by location index with a non-integer key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1655\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[0;32m-> 1656\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1658\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_ixs(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/miniconda3/envs/kera_lgbm/lib/python3.9/site-packages/pandas/core/indexing.py:1589\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1587\u001b[0m len_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis))\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m len_axis \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mlen_axis:\n\u001b[0;32m-> 1589\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle positional indexer is out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "year = int(year)\n",
    "year_next = year + 1\n",
    "month_act = [10, 11, 12]\n",
    "month_next = [1, 2, 3]\n",
    "if variable == 'geopotential':\n",
    "    way = '/work/FAC/FGSE/IDYST/tbeucler/default/raw_data/ECMWF/ERA5_hourly_PL/'\n",
    "else:\n",
    "    way = '/work/FAC/FGSE/IDYST/tbeucler/default/raw_data/ECMWF/ERA5/SL/'\n",
    "\n",
    "# Open and concatenate datasets\n",
    "if year == 1990:\n",
    "    dataset_act = open_and_concatenate(str(year), variable, month_next, way, level)\n",
    "    dataset_next = open_and_concatenate(str(year_next), variable, month_next, way, level)\n",
    "    dataset = xr.concat([dataset_act, dataset_next], dim='time')\n",
    "    dataset = dataset.chunk({'time': 10})\n",
    "elif year == 2021:\n",
    "    dataset = open_and_concatenate(str(year), variable, month_next, way, level)\n",
    "else:\n",
    "    dataset_act = open_and_concatenate(str(year), variable, month_act, way, level)\n",
    "    dataset_next = open_and_concatenate(str(year_next), variable, month_next, way, level)\n",
    "    dataset = xr.concat([dataset_act, dataset_next], dim='time')\n",
    "    dataset = dataset.chunk({'time': 10})\n",
    "\n",
    "# Determine the specific variable to extract\n",
    "specific_var = next(var for var in dataset.variables if var not in ['longitude', 'latitude', 'time', 'level'])\n",
    "\n",
    "# Import all tracks and convert dates\n",
    "dates = pd.read_csv(f'/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/curnagl/storms_start_end.csv', parse_dates=['start_date', 'end_date'])\n",
    "dates['year'] = dates['start_date'].dt.year\n",
    "\n",
    "# Find the indices for storms within the specified timeframe\n",
    "if year == 1990:\n",
    "    index_start_october = dates[(dates['start_date'].dt.month <= 3) & (dates['start_date'].dt.year == year)].index[0]\n",
    "    index_end_march = dates[(dates['end_date'].dt.month <= 3) & (dates['end_date'].dt.year == year_next)].index[0]\n",
    "elif year == 2021:\n",
    "    index_start_october = dates[(dates['start_date'].dt.month <= 3) & (dates['start_date'].dt.year == year)].index[0]\n",
    "    index_end_march = dates[(dates['end_date'].dt.year == 2021)].index[0]\n",
    "else:\n",
    "    index_start_october = dates[((dates['start_date'].dt.month >= 10) & (dates['start_date'].dt.year == year)) | ((dates['start_date'].dt.year == year_next) & (dates['start_date'].dt.month >= 1))].index[0]\n",
    "    index_end_march_first = dates[((dates['end_date'].dt.month <= 3) & (dates['end_date'].dt.year == year_next))].index\n",
    "    if len(index_end_march_first) > 0:\n",
    "        index_end_march = index_end_march_first[-1]\n",
    "    else:\n",
    "        index_end_march = dates[((dates['end_date'].dt.year == year) & (dates['end_date'].dt.month <= 12))].index[-1]\n",
    "\n",
    "# Process each storm\n",
    "for i in range(index_start_october, index_end_march+1):\n",
    "    track = pd.read_csv(f'/work/FAC/FGSE/IDYST/tbeucler/default/fabien/repos/curnagl/tc_irad_tracks/tc_1_hour/tc_irad_{i+1}_interp.txt')\n",
    "    start_date = dates.at[i, 'start_date']\n",
    "    end_date = dates.at[i, 'end_date']\n",
    "    storm_data = dataset[specific_var].sel(time=slice(start_date, end_date))\n",
    "\n",
    "    # Initialize lists to store statistics\n",
    "    stats = {'mean': [], 'min': [], 'max': [], 'std': []}\n",
    "\n",
    "    # Process each time step\n",
    "    for t_index, time_step in enumerate(storm_data.time):\n",
    "        #data_slice = storm_data.sel(time=time_step).values\n",
    "\n",
    "        # Extract coordinates for the current time step\n",
    "        lon_e_temp, lon_w_temp, lat_s_temp, lat_n_temp = track.iloc[t_index]\n",
    "        lon_test = np.asanyarray(storm_data.longitude[:])\n",
    "        lat_test = np.asanyarray(storm_data.latitude[:])\n",
    "\n",
    "        closest_lon_w = np.abs(lon_test - lon_w_temp).argmin()\n",
    "        closest_lon_e = np.abs(lon_test - lon_e_temp).argmin()\n",
    "        closest_lat_s = np.abs(lat_test - lat_s_temp).argmin()\n",
    "        closest_lat_n = np.abs(lat_test - lat_n_temp).argmin()\n",
    "\n",
    "        closest_lon_w_coor = lon_test[closest_lon_w]\n",
    "        closest_lon_e_coor = lon_test[closest_lon_e]\n",
    "        closest_lat_s_coor = lat_test[closest_lat_s]\n",
    "        closest_lat_n_coor = lat_test[closest_lat_n]\n",
    "\n",
    "        # Use .roll to handle the 0째/360째 boundary\n",
    "        if closest_lon_w_coor < 100 and closest_lon_e_coor > 100:\n",
    "            roll_shift = {'longitude': int(round(closest_lon_w_coor, 0)), 'longitude': int(round(closest_lon_e_coor, 0))}\n",
    "            storm_data_rolled = storm_data.roll(roll_shift, roll_coords=True)\n",
    "        else:\n",
    "            storm_data_rolled = storm_data\n",
    "\n",
    "        # Slice the dataset based on the rolled longitudes and latitudes\n",
    "        temp_ds_time = storm_data_rolled.isel(time=t_index)#[specific_var].isel(time=t_index)\n",
    "        temp_ds = temp_ds_time.sel(latitude=slice(closest_lat_n_coor, closest_lat_s_coor),\n",
    "                                    longitude=slice(closest_lon_e_coor, closest_lon_w_coor)).values\n",
    "\n",
    "        # Calculate statistics for the sliced data\n",
    "        step_stats = calculate_statistics(temp_ds)\n",
    "        for key in stats:\n",
    "            stats[key].append(step_stats[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new tests need to be done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
